{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9d8297-5e63-4692-ab16-9d00f2722ae2",
   "metadata": {},
   "source": [
    "# Pipeline Code\n",
    "Pipeline for inferring ecological interactions between microbes in treehole communities. Explanation of how the different steps of the pipeline work can be found below the pipeline, in the 'Pipeline explanation' section.\n",
    "\n",
    "Pipeline steps:\n",
    "1. Importing tree hole data\n",
    "2. Applying FlashWeave to infer co-occurrence networks\n",
    "3. Applying functionInk to infer communities from co-occurrence networks\n",
    "4. Visualising communities using Cytoscape\n",
    "5. Comparing co-occurrence network motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58cb943-0c0f-46ac-b160-a5bf3b660af4",
   "metadata": {},
   "source": [
    "## 1 - Importing tree hole data\n",
    "Importing the raw data, and converting it to the correct input format for FlashWeave.\n",
    "The code below is written in the Python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4073fd9d-3316-4375-9607-511a53663431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Importing data\n",
    "asv_table = pd.read_csv('../data/seqtable_readyforanalysis.csv', sep='\\t') # Importing ASV table\n",
    "metadata = pd.read_csv('../data/metadata_Time0D-7D-4M_May2022_wJSDpart_ext.csv', sep='\\t') # Importing metadata\n",
    "taxonomy_data = pd.read_csv('../data/taxa_wsp_readyforanalysis.csv', sep='\\t') # Importing taxonomy\n",
    "\n",
    "##### Cleaning the data, and giving it the correct format. #####\n",
    "\n",
    "# Getting rid of the samples belonging to experiment 4M:\n",
    "asv_table.reset_index(inplace=True) # Making the sample ID into a column for the ASV table\n",
    "asv_table.rename(columns={'index': 'sampleid'}, inplace=True) # renaming this new column to 'sampleid'\n",
    "asv_table = pd.merge(asv_table, metadata, on='sampleid') # Merging metadata and asv_table by 'sampleid' so that can remove 4M samples\n",
    "asv_table = asv_table[asv_table['Experiment'] != '4M'] # Taking rows which are not 4M samples\n",
    "\n",
    "# Splitting the ASV table into starting and final communities\n",
    "starting_asv_table = asv_table[asv_table['Experiment'] == '0D']\n",
    "final_asv_table = asv_table[asv_table['Experiment'] != '0D']\n",
    "\n",
    "# Splitting the metadata and ASV tables\n",
    "starting_metadata = starting_asv_table[['partition']] # The metadata only contains the column for the community classes\n",
    "final_metadata = final_asv_table[['partition']]\n",
    "columns_to_drop = ['sampleid', 'Name.2', 'Community', 'Species', 'replicate',\n",
    "       'BreakingBag', 'parent', 'Location', 'Experiment', 'Part_Time0D_17',\n",
    "'Community', 'Species', 'replicate',\n",
    "       'BreakingBag', 'parent', 'Location', 'Experiment', 'Part_Time0D_17',\n",
    "       'Part_Time0D_6', 'Part_Time4M_64', 'Part_Time7D_rep1_2',\n",
    "       'Part_Time7D_rep2_2', 'Part_Time7D_rep3_2', 'Part_Time7D_rep4_2',\n",
    "       'replicate.partition', 'partition', 'ExpCompact',\n",
    "       'exp.replicate.partition', 'exp.partition', 'Part_Time0D_6', 'Part_Time4M_64', 'Part_Time7D_rep1_2',\n",
    "       'Part_Time7D_rep2_2', 'Part_Time7D_rep3_2', 'Part_Time7D_rep4_2',\n",
    "       'replicate.partition', 'partition', 'ExpCompact',\n",
    "       'exp.replicate.partition', 'exp.partition'] # Columns to drop for ASV tables\n",
    "starting_asv_table = starting_asv_table.drop(columns=columns_to_drop) # The ASV tables are now in the correct input format for FlashWeave\n",
    "final_asv_table = final_asv_table.drop(columns=columns_to_drop)\n",
    "\n",
    "# Exporting to .tsv files, which can then be inputted to FlashWeave\n",
    "starting_metadata.to_csv('../data/starting_metadata.tsv', sep='\\t', index=False)\n",
    "final_metadata.to_csv('../data/final_metadata.tsv', sep='\\t', index=False)\n",
    "starting_asv_table.to_csv('../data/starting_asv_table.tsv', sep='\\t', index=False)\n",
    "final_asv_table.to_csv('../data/final_asv_table.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bdbd19-d0b0-4a4f-9ef2-6f8bb44b8f2f",
   "metadata": {},
   "source": [
    "## 2 - Applying FlashWeave\n",
    "Applying heterogeneous FlashWeave to the starting communities and the final communities with classes as a factor, and applying homogeneous FlashWeave to the starting communities and the final communities - ignoring community classes. This creates 4 networks in total. The code below is written in the Julia kernel.starting_netw_results = learn_network(starting_data_pstarting_netw_results = learn_network(starting_data_path, ath, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764febc-c29d-4993-a32f-1716aea6594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Applying FlashWeave #####\n",
    "# Co-occurrence network for starting communities, ignoring classes\n",
    "using FlashWeave\n",
    "starting_data_path = \"../data/starting_asv_table.tsv\"\n",
    "starting_netw_results = learn_network(starting_data_path, sensitive=true, heterogeneous=false)\n",
    "\n",
    "# Co-occurrence network for final communities, ignoring classes\n",
    "using FlashWeave\n",
    "final_data_path = \"../data/final_asv_table.tsv\"\n",
    "final_netw_results = learn_network(final_data_path, sensitive=true, heterogeneous=false)\n",
    "\n",
    "# Co-occurrence network for starting communities, taking into account classes\n",
    "using FlashWeave\n",
    "starting_classes_data_path = \"../data/starting_asv_table.tsv\"\n",
    "starting_classes_meta_data_path = \"../data/starting_metadata.tsv\"\n",
    "starting_classes_netw_results = learn_network(starting_classes_data_path, starting_classes_meta_data_path, sensitive=true, heterogeneous=true)\n",
    "\n",
    "# Co-occurrence network for final communities, taking into account classes\n",
    "using FlashWeave\n",
    "final_classes_data_path = \"../data/final_asv_table.tsv\"\n",
    "final_classes_meta_data_path = \"../data/final_metadata.tsv\"\n",
    "final_classes_netw_results = learn_network(final_classes_data_path, final_classes_meta_data_path, sensitive=true, heterogeneous=true)\n",
    "\n",
    "# Saving the networks\n",
    "save_network(\"../data/starting_network_output.edgelist\", starting_netw_results)\n",
    "save_network(\"../data/starting_classes_network_output.edgelist\", starting_classes_netw_results)\n",
    "save_network(\"../data/final_network_output.edgelist\", final_netw_results)\n",
    "save_network(\"../data/final_classes_network_output.edgelist\", final_classes_netw_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26c3ac-2730-4bc6-88ab-567d44c1158c",
   "metadata": {},
   "source": [
    "## 3 - Applying functionInk\n",
    "First, converting the FlashWeave outputs into the correct input format for functionInk. The below code is written in the Python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b98078-a64d-4ea5-bc58-2c3926b63611",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Converting the FlashWeave outputs into the correct input format for functionInk #####\n",
    "\n",
    "import pandas as pd # Importing Pandas again, as switched back to Python kernel\n",
    "\n",
    "# Removing the headers\n",
    "with open('../data/starting_network_output.edgelist', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "with open('../data/starting_network_output.edgelist', 'w') as f:\n",
    "    f.writelines(lines[2:])\n",
    "\n",
    "with open('../data/starting_classes_network_output.edgelist', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "with open('../data/starting_classes_network_output.edgelist', 'w') as f:\n",
    "    f.writelines(lines[2:])\n",
    "\n",
    "with open('../data/final_network_output.edgelist', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "with open('../data/final_network_output.edgelist', 'w') as f:\n",
    "    f.writelines(lines[2:])\n",
    "\n",
    "with open('../data/final_classes_network_output.edgelist', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "with open('../data/final_classes_network_output.edgelist', 'w') as f:\n",
    "    f.writelines(lines[2:])\n",
    "\n",
    "# Adding new headers and a column for the type of interaction (here, all assumed to be the same)\n",
    "starting_network_data = pd.read_csv(\"../data/starting_network_output.edgelist\", sep=\"\\t\", header=None, names=[\"ASV_A\", \"ASV_B\", \"Interaction\"])\n",
    "starting_network_data['Type'] = 1\n",
    "\n",
    "starting_classes_network_data = pd.read_csv(\"../data/starting_classes_network_output.edgelist\", sep=\"\\t\", header=None, names=[\"ASV_A\", \"ASV_B\", \"Interaction\"])\n",
    "starting_classes_network_data['Type'] = 1\n",
    "\n",
    "final_network_data = pd.read_csv(\"../data/final_network_output.edgelist\", sep=\"\\t\", header=None, names=[\"ASV_A\", \"ASV_B\", \"Interaction\"])\n",
    "final_network_data['Type'] = 1\n",
    "\n",
    "final_classes_network_data = pd.read_csv(\"../data/final_classes_network_output.edgelist\", sep=\"\\t\", header=None, names=[\"ASV_A\", \"ASV_B\", \"Interaction\"])\n",
    "final_classes_network_data['Type'] = 1\n",
    "\n",
    "# Outputting as .tsv files\n",
    "starting_network_data.to_csv('../data/starting_network_data.tsv', sep='\\t', index=False, header=['#ASV_A', 'ASV_B', 'Interaction', 'Type'])\n",
    "starting_classes_network_data.to_csv('../data/starting_classes_network_data.tsv', sep='\\t', index=False, header=['#ASV_A', 'ASV_B', 'Interaction', 'Type'])\n",
    "final_network_data.to_csv('../data/final_network_data.tsv', sep='\\t', index=False, header=['#ASV_A', 'ASV_B', 'Interaction', 'Type'])\n",
    "final_classes_network_data.to_csv('../data/final_classes_network_data.tsv', sep='\\t', index=False, header=['#ASV_A', 'ASV_B', 'Interaction', 'Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cf933d-5e2d-430d-9e02-8f25c709a6da",
   "metadata": {},
   "source": [
    "Applying functionInk to each of the 4 networks. The below code is written in the Python kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f5abe-a023-49a9-8682-ee6661411977",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Applying functionInk #####\n",
    "\n",
    "import os # Importing os package\n",
    "os.chdir('../code/functionInk') # Moving from the directory in which this notebook is found into the root of the functionInk repository\n",
    "\n",
    "# The first step to the pipeline - computing similarities between nodes\n",
    "!./NodeSimilarity.pl -w 1 -d 0 -t 0 -f ../../data/starting_network_data.tsv\n",
    "!./NodeSimilarity.pl -w 1 -d 0 -t 0 -f ../../data/starting_classes_network_data.tsv\n",
    "!./NodeSimilarity.pl -w 1 -d 0 -t 0 -f ../../data/final_network_data.tsv\n",
    "!./NodeSimilarity.pl -w 1 -d 0 -t 0 -f ../../data/final_classes_network_data.tsv\n",
    "\n",
    "# The second step - clustering nodes using the similarity metrics calculated\n",
    "!./NodeLinkage.pl -fn ../../data/starting_network_data.tsv -fs Nodes-Similarities_starting_network_data.tsv\n",
    "!./NodeLinkage.pl -fn ../../data/starting_classes_network_data.tsv -fs Nodes-Similarities_starting_classes_network_data.tsv\n",
    "!./NodeLinkage.pl -fn ../../data/final_network_data.tsv -fs Nodes-Similarities_final_network_data.tsv\n",
    "!./NodeLinkage.pl -fn ../../data/final_classes_network_data.tsv -fs Nodes-Similarities_final_classes_network_data.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1a70c-2489-4606-a182-088dc13d93f0",
   "metadata": {},
   "source": [
    "Extracting the partition densities. Please switch to the R kernel to run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78265bd7-98d3-4b31-8dd2-9f4c904502bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Sourcing function that extracts partition densities #####\n",
    "library(ggplot2) # Loading ggplot\n",
    "source(\"functionInk/scripts/analysis_R/extractPartDensity.R\") # Sourcing the function that extracts the partition densities\n",
    "setwd(\"functionInk\") # Moving to the functionInk repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac72143-3a83-4f95-950e-632f8143d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Extracting partition densities #####\n",
    "# Importing the partition histories and cleaning them\n",
    "hist_comp_starting=read.table(file=\"HistCompact-NL_Average_NoStop_starting_network_data.tsv\")\n",
    "colnames(hist_comp_starting) <- as.character(unlist(hist_comp_starting[1, ]))\n",
    "hist_comp_starting <- hist_comp_starting[-1, ]\n",
    "columns_to_convert <- c(\"Step\", \"Similarity\", \"Density\", \"DensityInt\", \"DensityExt\", \"NumNodesA\", \"NumEdgesA\", \n",
    "                       \"NumNodesB\", \"NumEdgesB\", \"NumNodesAB\", \"NumEdgesAB\", \"NumIntNodesA\", \"NumIntNodesB\",\n",
    "                       \"NumExtNodesA\", \"NumExtNodesB\", \"NumIntNodesAB\", \"NumExtNodesAB\", \"NumIntEdgesA\",\n",
    "                       \"NumIntEdgesB\", \"NumExtEdgesA\", \"NumExtEdgesB\", \"NumIntEdgesAB\", \"NumExtEdgesAB\",\n",
    "                       \"NcumInt\", \"NcumExt\", \"Ncum\")\n",
    "hist_comp_starting[columns_to_convert] <- lapply(hist_comp_starting[columns_to_convert], as.numeric)\n",
    "\n",
    "hist_comp_starting_classes=read.table(file=\"HistCompact-NL_Average_NoStop_starting_classes_network_data.tsv\")\n",
    "colnames(hist_comp_starting_classes) <- as.character(unlist(hist_comp_starting_classes[1, ]))\n",
    "hist_comp_starting_classes <- hist_comp_starting_classes[-1, ]\n",
    "hist_comp_starting_classes[columns_to_convert] <- lapply(hist_comp_starting_classes[columns_to_convert], as.numeric)\n",
    "\n",
    "hist_comp_final=read.table(file=\"HistCompact-NL_Average_NoStop_final_network_data.tsv\")\n",
    "colnames(hist_comp_final) <- as.character(unlist(hist_comp_final[1, ]))\n",
    "hist_comp_final <- hist_comp_final[-1, ]\n",
    "hist_comp_final[columns_to_convert] <- lapply(hist_comp_final[columns_to_convert], as.numeric)\n",
    "\n",
    "hist_comp_final_classes=read.table(file=\"HistCompact-NL_Average_NoStop_final_classes_network_data.tsv\")\n",
    "colnames(hist_comp_final_classes) <- as.character(unlist(hist_comp_final_classes[1, ]))\n",
    "hist_comp_final_classes <- hist_comp_final_classes[-1, ]\n",
    "hist_comp_final_classes[columns_to_convert] <- lapply(hist_comp_final_classes[columns_to_convert], as.numeric)\n",
    "\n",
    "\n",
    "# Calculating partition densities, plotting them, and moving the plot into results\n",
    "print(\"Starting network partition densities:\")\n",
    "part_density_starting=extractPartDensity(hist.comp=hist_comp_starting, plot = TRUE)\n",
    "system(paste(\"mv\", \"figures/Plot_PartitionDensityVsStep.pdf\", \"../../results/starting_Plot_PartitionDensityVsStep.pdf\"))\n",
    "print(\"Step of the clustering in which the maximum of the total partition density was found: \")\n",
    "part_density_starting$total_dens_step\n",
    "print(\"Step of the clustering in which the maximum of the internal partition density was found \")\n",
    "part_density_starting$int_dens_step\n",
    "print(\"Step of the clustering in which the maximum of the external partition density was found: \")\n",
    "part_density_starting$ext_dens_step\n",
    "\n",
    "print(\"Starting classes network partition densities:\")\n",
    "part_density_starting_classes=extractPartDensity(hist.comp=hist_comp_starting_classes, plot = TRUE)\n",
    "system(paste(\"mv\", \"figures/Plot_PartitionDensityVsStep.pdf\", \"../../results/starting_classes_Plot_PartitionDensityVsStep.pdf\"))\n",
    "print(\"Step of the clustering in which the maximum of the total partition density was found: \")\n",
    "part_density_starting_classes$total_dens_step\n",
    "print(\"Step of the clustering in which the maximum of the internal partition density was found \")\n",
    "part_density_starting_classes$int_dens_step\n",
    "print(\"Step of the clustering in which the maximum of the external partition density was found: \")\n",
    "part_density_starting_classes$ext_dens_step\n",
    "\n",
    "print(\"Final network partition densities:\")\n",
    "part_density_final=extractPartDensity(hist.comp=hist_comp_final, plot = TRUE)\n",
    "system(paste(\"mv\", \"figures/Plot_PartitionDensityVsStep.pdf\", \"../../results/final_Plot_PartitionDensityVsStep.pdf\"))\n",
    "print(\"Step of the clustering in which the maximum of the total partition density was found: \")\n",
    "part_density_final$total_dens_step\n",
    "print(\"Step of the clustering in which the maximum of the internal partition density was found \")\n",
    "part_density_final$int_dens_step\n",
    "print(\"Step of the clustering in which the maximum of the external partition density was found: \")\n",
    "part_density_final$ext_dens_step\n",
    "\n",
    "print(\"Final classes network partition densities:\")\n",
    "part_density_final_classes=extractPartDensity(hist.comp=hist_comp_final_classes, plot = TRUE)\n",
    "system(paste(\"mv\", \"figures/Plot_PartitionDensityVsStep.pdf\", \"../../results/final_classes_Plot_PartitionDensityVsStep.pdf\"))\n",
    "print(\"Step of the clustering in which the maximum of the total partition density was found: \")\n",
    "part_density_final_classes$total_dens_step\n",
    "print(\"Step of the clustering in which the maximum of the internal partition density was found \")\n",
    "part_density_final_classes$int_dens_step\n",
    "print(\"Step of the clustering in which the maximum of the external partition density was found: \")\n",
    "part_density_final_classes$ext_dens_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3dd5a1-b24d-45aa-aa2f-9916558a70fd",
   "metadata": {},
   "source": [
    "Obtaining communities. Please switch to the Python kernel to run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e5e1f-ac6a-4c61-8c72-1c6e52489b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Importing os again, as switched back to Python kernel\n",
    "os.chdir('functionInk') # Moving from the directory in which this notebook is found into the root of the functionInk repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa30c2-39ca-4ad7-9827-b192108fbf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Running the clustering until the step at which the total partition density is reached #####\n",
    "!./NodeLinkage.pl -fn ../../data/starting_network_data.tsv -fs Nodes-Similarities_starting_network_data.tsv -s step -v 857\n",
    "!./NodeLinkage.pl -fn ../../data/starting_classes_network_data.tsv -fs Nodes-Similarities_starting_classes_network_data.tsv -s step -v 214\n",
    "!./NodeLinkage.pl -fn ../../data/final_network_data.tsv -fs Nodes-Similarities_final_network_data.tsv -s step -v 861\n",
    "!./NodeLinkage.pl -fn ../../data/final_classes_network_data.tsv -fs Nodes-Similarities_final_classes_network_data.tsv -s step -v 328"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033168c2-4478-4f87-af2c-f203dbbc95d7",
   "metadata": {},
   "source": [
    "Moving the output files from the functionInk pipeline from the data directory into the results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe89e0c-fab7-4740-a1b6-7a1d435b1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Moving the output files from the functionInk pipeline #####\n",
    "# Moving the node similarity files to the results folder\n",
    "source_file = 'Nodes-Similarities_starting_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'Nodes-Similarities_starting_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'Nodes-Similarities_final_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'Nodes-Similarities_final_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "# Moving the compact node clustering history files\n",
    "source_file = 'HistCompact-NL_Average_NoStop_starting_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'HistCompact-NL_Average_NoStop_starting_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'HistCompact-NL_Average_NoStop_final_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'HistCompact-NL_Average_NoStop_final_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "# Moving the detailed node clustering history files with the stop\n",
    "source_file = 'HistExtend-NL_Average_NoStop_starting_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'HistExtend-NL_Average_NoStop_starting_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'HistExtend-NL_Average_NoStop_final_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'HistExtend-NL_Average_NoStop_final_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "# Moving the compact node clustering history files with the stop\n",
    "source_file = 'HistCompact-NL_Average_StopStep-857_starting_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'HistCompact-NL_Average_StopStep-214_starting_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'HistCompact-NL_Average_StopStep-861_final_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'HistCompact-NL_Average_StopStep-328_final_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "# Moving the extended node clustering history files with the stop\n",
    "source_file = 'HistExtend-NL_Average_StopStep-857_starting_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'HistExtend-NL_Average_StopStep-214_starting_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'HistExtend-NL_Average_StopStep-861_final_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'HistExtend-NL_Average_StopStep-328_final_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "# Moving the cluster description files\n",
    "source_file = 'Clusters-NL_Average_StopStep-857_starting_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'Clusters-NL_Average_StopStep-214_starting_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'Clusters-NL_Average_StopStep-861_final_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'Clusters-NL_Average_StopStep-328_final_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "# Moving the file describing the cluster to which each node belongs\n",
    "source_file = 'Partition-NL_Average_StopStep-857_starting_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'Partition-NL_Average_StopStep-214_starting_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'Partition-NL_Average_StopStep-861_final_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'Partition-NL_Average_StopStep-328_final_classes_network_data.tsv'\n",
    "destination_directory = '../../results'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9f3fe-3521-4345-b59a-4324927529a7",
   "metadata": {},
   "source": [
    "## 4 - Visualisation in Cytoscape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1fafd-03ad-4c59-a760-70ffd93d08e1",
   "metadata": {},
   "source": [
    "Cytoscape visualisations of the four networks can be found in the results folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8fbc3f-c792-4d37-97f2-ee74834ed52d",
   "metadata": {},
   "source": [
    "## 5 - Comparing co-occurrence network structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21668b49-581b-47c5-9cfa-f3e69f5576f3",
   "metadata": {},
   "source": [
    "There are different methods of comparing co-occurrence network structure:\n",
    "- Compare whether a given pair of ASVs are found in the same cluster, between networks, for all ASV pairs\n",
    "- Compare whether a given pair of ASVs has a similar correlation value between networks, for all ASV pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c71f8-0803-4c3d-836e-3c39ea4d756b",
   "metadata": {},
   "source": [
    "#### 5.1 Comparing whether ASVs have similar links between networks\n",
    "Comparing the strength of links between ASVs between networks. For each network, each pair of ASVs has a correlation value (the correlation between the two ASVs in the pair). Intending to compare whether ASVs are correlated similarly in both networks. The below code is written in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490eaa6-1b8e-47a9-8946-4c77166f6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Can alternatively use functionInk's node similarities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1621786a-9748-4d92-9373-420b4cde9403",
   "metadata": {},
   "source": [
    "#### 5.2 Comparing clusters between networks\n",
    "Comparing the clusters between the networks. The main way of comparing clusters seems to be by comparing whether a given pair of ASVs are found within the same cluster within each network. For example, if ASV_A and ASV_B are found in the same cluster in both the starting_classes network and the final_classes network, then this is a similarity between the networks. Perhaps later on it will be possible to identify larger motifs - for example, if ASV_A is connected to ASV_B and ASV_B is connected to ASV_B, within both networks, then both networks share this larger motif.\n",
    "\n",
    "Below, I use Cohen's Kappa to compare whether or not each pair of ASVs are found within the same cluster within each network. First, I construct a table containing information about whether each ASV pair is in the same cluster, for each network\n",
    "The code below is written in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22d62d-29b1-4363-b616-48bf660022d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data frame that will contain a row for each ASV pair, and a column for each network.\n",
    "# Each cell will indicate whether or not the ASV pair are found in the same cluster.\n",
    "all_pair_cluster_data <- data.frame(matrix(nrow = 0, ncol = 5))\n",
    "colnames(all_pair_cluster_data) <- c(\"ASV.pair\", \"starting.same.cluster\", \"starting.classes.same.cluster\",\n",
    "                                \"final.same.cluster\", \"final.classes.same.cluster\")\n",
    "\n",
    "# Data frame containing a row for each pair and a column for whether or not that pair is found within the same cluster in the starting network.\n",
    "starting_pair_cluster_data <- data.frame(matrix(nrow = 0, ncol = 5))\n",
    "colnames(starting_pair_cluster_data) <- c(\"ASV.pair\", \"starting.same.cluster\")\n",
    "\n",
    "# Importing the cluster data for the starting network - this indicates which cluster each ASV is in within the starting network.\n",
    "starting_cluster_data <- read.table(\"../results/Partition-NL_Average_StopStep-857_starting_network_data.tsv\", sep = \"\\t\", header = FALSE)\n",
    "\n",
    "# Looping across every possible pair of ASVS to determine whether they are in the same class.\n",
    "for (i in 1:nrow(starting_cluster_data)){\n",
    "    #print(\"yo tengo\")\n",
    "    for (j in 1:nrow(starting_cluster_data)){\n",
    "        if (i != j) {\n",
    "            if (starting_cluster_data[i, 2] == starting_cluster_data[j, 2]) {\n",
    "                pair <- paste(starting_cluster_data[i, 1], starting_cluster_data[j, 1], sep = \".\")\n",
    "                new_row <- c(pair, \"yes\")\n",
    "                starting_pair_cluster_data <- rbind(starting_pair_cluster_data, new_row)\n",
    "                print(\"Chungus\")\n",
    "                }\n",
    "            else if (starting_cluster_data[i, 2] != starting_cluster_data[j, 2]) {\n",
    "                pair <- paste(starting_cluster_data[i, 1], starting_cluster_data[j, 1], sep = \".\")\n",
    "                new_row <- c(pair, \"no\")\n",
    "                starting_pair_cluster_data <- rbind(starting_pair_cluster_data, new_row)\n",
    "                print(\"Nonegus\")\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "starting_pair_cluster_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca17be-7b0a-4809-8954-2c912c8170b3",
   "metadata": {},
   "source": [
    "Pipeline Explanation\n",
    "-----\n",
    "\n",
    "### Data set\n",
    "\n",
    "The data set includes samples from 275 tree holes. There is 1 initial sample from each tree hole (275 total) as well as 4 replicate samples for each of the tree holes from after about a week (1100 total). The data is in the form of an ASV table, where the counts of ASVs are listed. The samples each represent a community, and they have been grouped into community classe within both the starting samples and each of the replicate sets of the final samples, based upon beta diversity dissimilarity.\n",
    "\n",
    "### What is FlashWeave?\n",
    "FlashWeave is considered to be the gold standard for inferring co-occurrences between microbial ASVs. It has shown an improved accuracy and performance upon synthetic data, when compared to other commonly used methods such as SparCC and SpiecEasi.\n",
    "\n",
    "### How does FlashWeave work?\n",
    "For each ASV, FlashWeave identifies its directly associated neighbouring ASVs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
