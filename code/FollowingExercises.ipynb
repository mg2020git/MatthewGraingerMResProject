{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4682d2e0-1099-454c-9284-28f1e8af81bc",
   "metadata": {},
   "source": [
    "# Following the exercises from https://pawarlab.slack.com/files/U06GE8MMN49/F07866US999/proposal_pipeline_randomforest_mgrainger_mscthesis.odt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49a375-8b02-4754-8ab2-bb79515ff5a0",
   "metadata": {},
   "source": [
    "## Importing data and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af502d0-27df-4d96-ad9f-d7f69b6eff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "asv_table <- read.csv(\"../data/seqtable_readyforanalysis.csv\", sep = \"\\t\")\n",
    "meta_data <- read.csv(\"../data/metadata_Time0D-7D-4M_May2022_wJSDpart_ext.csv\", sep = \"\\t\")\n",
    "cluster_data <- read.csv(\"../data/max_tot_ext_network_table.tsv\", sep = \"\\t\")\n",
    "function_data <- read.csv(\"../data/20151016_Functions_remainder.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c04cff-49d3-46e7-b3b5-d4ddffc3eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8987d-d56c-426d-8b74-f4b6a779db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24724db1-3b06-4764-ac37-37ffc230f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "asv_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a6dd5f-bae2-4be7-970e-5d47b6617d78",
   "metadata": {},
   "source": [
    "## Correcting data\n",
    "Steps:\n",
    "1. Change units of function data, normalise, and take log.\n",
    "2. Add column for the logarithm of the total abundance of each sample in the ASV table, as a way of later controlling the number of reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03fa54-f25c-4e6c-a036-5797be956ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing units\n",
    "function_data$mgCO2.7 <- function_data$mgCO2.7 * 1000 # Converting CO2 from miligr. to microgr.\n",
    "names(function_data)[names(function_data) == \"mgCO2.7\"] <- \"μgCO2.7\" # Changing column name accordingly\n",
    "function_data$ATP7 <- function_data$ATP7 / 1000 # Changing nanomolar to micromolar\n",
    "function_data$ATP14 <- function_data$ATP14 / 1000\n",
    "\n",
    "# Normalising by number of cells\n",
    "function_data$ATP7.norm <- function_data$ATP7 / function_data$CPM7\n",
    "function_data$mG7.norm <- function_data$mG7 / function_data$CPM7\n",
    "function_data$mN7.norm <- function_data$mN7 / function_data$CPM7\n",
    "function_data$mX7.norm <- function_data$mX7 / function_data$CPM7\n",
    "function_data$mP7.norm <- function_data$mP7 / function_data$CPM7\n",
    "function_data$μgCO2.7.norm <- function_data$μgCO2.7 / function_data$CPM7\n",
    "\n",
    "# Taking log of normalised functions and cell count\n",
    "function_data$log.ATP7.norm <- log(function_data$ATP7.norm + 0.00001)\n",
    "function_data$log.mG7.norm <- log(function_data$mG7.norm + 0.00001)\n",
    "function_data$log.mN7.norm <- log(function_data$mN7.norm + 0.00001)\n",
    "function_data$log.mX7.norm <- log(function_data$mX7.norm + 0.00001)\n",
    "function_data$log.mP7.norm <- log(function_data$mP7.norm + 0.00001)\n",
    "function_data$log.μgCO2.7.norm <- log(function_data$μgCO2.7.norm + 0.00001)\n",
    "\n",
    "# Adding column for log of total abundance of each sample\n",
    "asv_table$log.reads <- log(rowSums(asv_table))\n",
    "\n",
    "# Writing modified data to files\n",
    "write.csv(function_data, \"../data/20151016_Functions_remainder_corrected.csv\", row.names = FALSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b9276-a7ab-46be-b55d-e8a2aa872605",
   "metadata": {},
   "outputs": [],
   "source": [
    "asv_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd99b86-3f13-4cad-85ff-11da76fce356",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Random forest code copied from random_forest_cfl.R and then modified to match the exercises.\n",
    "Exercise 1 includes the following 8 random forests:\n",
    "1. ASVs and reads as predictors, log normal atp response. Training data is one replicate, testing data is the other 3.\n",
    "2. ASVs and reads as predictors, log normal xylo response. Training data is one replicate, testing data is the other 3.\n",
    "3. ASVs and reads as predictors, log normal atp response. Training data is across all replicates, testing data is across all replicates.\n",
    "4. ASVs and reads as predictors, log normal xylo response. Training data is across all replicates, testing data is across all replicates.\n",
    "5. Same as all of these 4 but with clusters instead of ASVs.\n",
    "\n",
    "Exercise 1A Steps:\n",
    "1. Making predictor data frame with the ASV abundances of the final experiment (all 4 replicates) and the logarithm of the total abundance of each sample.\n",
    "2. Making univariate response data frames. One has the logarithm of normalised ATP7, and one has the logarithm of normalised xylosidase.\n",
    "3. Conduct 2 univariate random forest regressions, one for each of these response data frames.\n",
    "4. Identify the most important ASVs contributing to the function as those with high \n",
    "mse_reduction and Gini index. See this tutorial (multi-way importance): \n",
    "https://modeloriented.github.io/randomForestExplainer/articles/randomForestExplainer.html. \n",
    "5. Check where in network these ASVs are.\n",
    "6. Investigate interactions between a set of important ASVs and compare them with your \n",
    "network (see in the above tutorial “variable interactions”). Are RF interactions in \n",
    "correspondence with FlashWeave associations?\n",
    "\n",
    "Exercise 1B Steps:\n",
    "1. Making multivariate response data frame with the logarithm of the following normalized functions: ATP, X,\n",
    "G, P, N.\n",
    "2. Same thing as in 1A, except with multivariate RF regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c910c2-9ee0-4abd-8d49-32041d74c177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983cfa5-a451-4751-8fe1-ed5b11094c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#     random_forest_cfl.R  \n",
    "##################################\n",
    "# Author: Alberto Pascual-García\n",
    "# Copyright (c)  Alberto Pascual-García,  2024\n",
    "# Web:  apascualgarcia.github.io\n",
    "# \n",
    "# Date: 2024-05-13\n",
    "# Script Name: random_forest.R   \n",
    "\n",
    "\n",
    "rm(list = ls()) # Clear workspace\n",
    "######### START EDITING ------------\n",
    "\n",
    "# --- Random forest parameters\n",
    "optimization=1 # Should RF parameters be optimized? (=1)\n",
    "run_RF=1 # Should RF be run (=1) or just read from file (=0)\n",
    "type_RF=\"regression\" # Should RF be a \"classification\" or a \"regression\"\n",
    "ntree.min=1000 # number of trees, mandatory if optimization = 0 (and only used in that case)\n",
    "mtry.min=\"default\" # number of variables randomly selected to build the trees. Fix to \"default\" \n",
    "# if you don't have an informed guess\n",
    "partial.plots=0 # should partial plots be generated (=1), fix to 0 otherwise. It will\n",
    "# generate plots for the 10 most important variables. This is controlled by\n",
    "# variable Nsel in the section plots below\n",
    "select.func=\"ATP\" # determine the function you want to predict. Name should match those from\n",
    "                # the column name of the function data frame.\n",
    "select.cond=c(\"7\") # determine the function conditions you want to predict. Names should \n",
    "                      #match those used to differentiate the different conditions within the function df.\n",
    "funcxcond=1 #number of total combinations of function and condition \n",
    "log.func=TRUE #logical indicating whether the values of the function should be logged for the analysis\n",
    "\n",
    "\n",
    "# SET WORKING DIRECTORY -----------------------------\n",
    "\n",
    "select.class = select.func\n",
    "\n",
    "# --- Input and output files\n",
    "file.ASV = \"seqtable_readyforanalysis.csv\"\n",
    "file.taxa = \"taxa_wsp_readyforanalysis.csv\"\n",
    "file.Meta = \"metadata_Time0D-7D-4M_May2022_wJSDpart_ext.csv\"\n",
    "file.func = \"20151016_Functions_remainder.csv\"\n",
    "\n",
    "# --- Directories\n",
    "# this.dir=strsplit(rstudioapi::getActiveDocumentContext()$path, \"/src/\")[[1]][1] # don't edit, just comment it if problems...\n",
    "this.dir=strsplit(rstudioapi::getActiveDocumentContext()$path, \"/code/\")[[1]][1]\n",
    "dirSrc=paste(this.dir,\"/code/\",sep=\"\") # Directory where the code is\n",
    "dirASV=paste(this.dir,\"/data/\",sep=\"\") # Dir of ASV table\n",
    "dirMeta=paste(this.dir,\"/data/\",sep=\"\") # Dir of metadata\n",
    "dirFunc=paste(this.dir,\"/data/\",sep=\"\") #Dir of response function data frame\n",
    "dirOut=paste(this.dir,\"/results/fnl_random_forest\",sep=\"\") # Dir of output data\n",
    "\n",
    "# --- Packages\n",
    "\n",
    "packages <- c(\"tidyverse\", \"stringr\", \"ggplot2\",\n",
    "              \"caret\", # additional RF functions\n",
    "              \"randomForest\", \"randomForestExplainer\") # list of packages to load\n",
    "\n",
    "scripts <- c(\"clean_ASV_table.R\") # list of functions to load\n",
    "\n",
    "# --- Reshaping data parameters\n",
    "nreads = 10000 # minimum number of reads to consider a sample\n",
    "exclude_exp = c(\"4M\") # A vector of characters with the experiments that should be excluded\n",
    "match_exp = TRUE # Set to true if only starting communities that were resurrected should be included\n",
    "output.label = \"Time0D_7D_matched\" \n",
    "\n",
    "###### STOP EDITING -------------\n",
    "\n",
    "# INSTALL PACKAGES & LOAD LIBRARIES -----------------\n",
    "cat(\"INSTALLING PACKAGES & LOADING LIBRARIES... \\n\\n\", sep = \"\")\n",
    "\n",
    "n_packages <- length(packages) # count how many packages are required\n",
    "\n",
    "new.pkg <- packages[!(packages %in% installed.packages())] # determine which packages aren't installed\n",
    "\n",
    "# install missing packages\n",
    "if(length(new.pkg)){\n",
    "  install.packages(new.pkg)\n",
    "}\n",
    "\n",
    "# load all requried libraries\n",
    "for(n in 1:n_packages){\n",
    "  cat(\"Loading Library #\", n, \" of \", n_packages, \"... Currently Loading: \", packages[n], \"\\n\", sep = \"\")\n",
    "  lib_load <- paste(\"library(\\\"\",packages[n],\"\\\")\", sep = \"\") # create string of text for loading each library\n",
    "  eval(parse(text = lib_load)) # evaluate the string to load the library\n",
    "}\n",
    "# SOURCE FUNCTIONS ---------\n",
    "setwd(dirSrc)\n",
    "n_scripts <- length(scripts) # count how many packages are required\n",
    "\n",
    "for(n in 1:n_scripts){\n",
    "  cat(\"Loading script #\", n, \" of \", n_scripts, \"... Currently Loading: \", scripts[n], \"\\n\", sep = \"\")\n",
    "  lib_load <- paste(\"source(\\\"\",scripts[n],\"\\\")\", sep = \"\") # create string of text for loading each library\n",
    "  eval(parse(text = lib_load)) # evaluate the string to load the library\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# READ INPUT FILES ----------\n",
    "# --- Read ASVs table\n",
    "setwd(dirASV)\n",
    "ASV.table=read.table(file = file.ASV, sep=\"\\t\")\n",
    "colnames(ASV.table)[1:5]\n",
    "rownames(ASV.table)[1:5]\n",
    "dim(ASV.table)\n",
    "head(ASV.table)[1:5,1:5]\n",
    "\n",
    "# ..... read metadata. Samples present in metadata were those passing the filtering\n",
    "setwd(dirMeta)\n",
    "sample_md <-read.table(file = file.Meta, sep=\"\\t\", header=TRUE)\n",
    "head(sample_md)[1:5,1:5]\n",
    "\n",
    "# --- Read function data frame\n",
    "setwd(dirMeta)\n",
    "data.func <- read.csv(file = file.func)\n",
    "head(data.func)[1:5,1:5]\n",
    "\n",
    "# # CLEAN DATA ---------- \n",
    "# \n",
    "# # Clean data   ------\n",
    "# clean.data.list = clean_ASV_table(ASV.table,sample_md,match.exp = T)\n",
    "# \n",
    "# ASV.table = clean.data.list$ASV.table\n",
    "# sample_md = clean.data.list$sample_md\n",
    "# \n",
    "# head(ASV.table)[1:5,1:5]\n",
    "\n",
    "\n",
    "# BUILD REGRESSION DATA ---------- \n",
    "\n",
    "# .... Build the table of predictors, first ASVs (abundances or presence/absence).\n",
    "# Here we will have a table with parent's ids and ASVs repeated six times\n",
    "# we will regress these variables to the response (function value)\n",
    "id.t7 = grep(\"7D\", sample_md$Experiment)\n",
    "samples.t7 = as.character(sample_md$sampleid[id.t7])\n",
    "\n",
    "xIn.tmp = ASV.table[samples.t7, ]\n",
    "xIn.tmp$counts = rowSums(xIn.tmp) # add the sum of the counts for each sample as an additional predictor\n",
    "xIn.tmp$counts = log(xIn.tmp$counts) # log it to avoid having too high numbers\n",
    "xIn.tmp$sampleid = rownames(xIn.tmp) # create a new column, rownames cannot be repeated\n",
    "\n",
    "xIn = xIn.tmp[rep(seq_len(nrow(xIn.tmp)), funcxcond), ] # we repeat xIn.tmp as many times as combination\n",
    "                                                        # between function and condition are available\n",
    "\n",
    "rownames(xIn) = seq(1,dim(xIn)[1]) \n",
    "xIn = xIn[,c(dim(xIn)[2],1:(dim(xIn)[2]-1))] # reorder\n",
    "head(xIn)[1:5,1:5]\n",
    "\n",
    "\n",
    "# # .... Now add the replicate as an additional envir predictor   ###We consider each replicate as an independent sample\n",
    "# replicate = c(rep(1, length(id.t0)), rep(2, length(id.t0)),\n",
    "#               rep(3, length(id.t0)),rep(4, length(id.t0)))\n",
    "# xIn$replicate = replicate\n",
    "\n",
    "# We add as additional predictors variables that give information about the function and the conditions\n",
    "# But first, we have to adapt the response data frame data.func\n",
    "functions.list <- colnames(data.func)\n",
    "data.func.x <- data.frame(data.func[,grep(select.func, functions.list), drop = FALSE])\n",
    "\n",
    "# .... Now add the functional conditions as an additional predictor.\n",
    "# .... In our case the sole condition is growth time\n",
    "functions.list <- colnames(data.func.x)\n",
    "data.func.x <- data.func.x[,grep(select.cond, functions.list), drop = FALSE]\n",
    "#############################################################################################################\n",
    "\n",
    "# Change data.func.x into the long format\n",
    "source(paste0(dirSrc, \"reshape_df_to_ggplot.R\"))\n",
    "data.func.x$sampleid <- paste0(data.func$Community, \".\", data.func$Replicate)\n",
    "sample.id <- intersect(data.func.x$sampleid, xIn.tmp$sampleid) \n",
    "# IMPORTANT NOTE: Only 1035/1402 samples in xIn.tmp are present in data.func\n",
    "#which(data.func.x$sampleid == sample.id)\n",
    "data.func.x <- data.func.x[data.func.x$sampleid %in% sample.id, ]\n",
    "\n",
    "x = data.func.x$sampleid\n",
    "\n",
    "\n",
    "ex.df = data.func.x[,which(colnames(data.func.x) == \"sampleid\"), drop = FALSE]\n",
    "col.names <- c(\"x\")\n",
    "\n",
    "for (i in seq(1, dim(data.func.x)[2])) {\n",
    "\n",
    "  if (colnames(data.func.x)[i] == \"sampleid\") {\n",
    "    \n",
    "  } else {\n",
    "    col.names <- c(col.names, paste0(\"y\", i))\n",
    "    ex.df = cbind(ex.df, data.func.x[i])\n",
    "  }\n",
    "}\n",
    "\n",
    "colnames(ex.df) <- col.names\n",
    "\n",
    "x.vec <- col.names[1]\n",
    "y.vec <- col.names[-1]\n",
    "\n",
    "char1 = rep(select.func, each = length(select.cond))\n",
    "char2 = rep(select.cond, times = length(select.func))\n",
    "  \n",
    "xIn.2 = reshape_df_to_ggplot(ex.df,x.vec = x.vec,y.vec = y.vec,\n",
    "                                              char.list = list(char1,\n",
    "                                                               char2))\n",
    "\n",
    "colnames(xIn.2) <- c(\"sampleid\", \"y.in\", \"v1\", \"v2\")\n",
    "\n",
    "\n",
    "#Combine all the predictors from xIn and xIn.2\n",
    "xIn <- xIn[xIn$sampleid %in% sample.id, ]\n",
    "\n",
    "xIn <- data.frame(xIn, xIn.2$v1, xIn.2$v2)\n",
    "\n",
    "# .... Extract response variable\n",
    "# # Each sample will have a function response according to the function and condition variables\n",
    "\n",
    "yIn = xIn.2$y.in\n",
    "yIn = as.numeric(yIn)\n",
    "\n",
    "if (log.func == TRUE) {\n",
    "  yIn = yIn + 0.0001 #We add an insignificant value to all of the samples to prevent \n",
    "  #the appearance of -Inf values after calculating the log\n",
    "  yIn = log(yIn)\n",
    "  \n",
    "} else {\n",
    "  \n",
    "}\n",
    "\n",
    "#quantile(yIn)\n",
    "\n",
    "# ... Finally, drop sampleid names\n",
    "xIn = subset(xIn, select = -c(sampleid))\n",
    "head(xIn)[1:5,1:5]\n",
    "\n",
    "# RANDOM FOREST computation -------------\n",
    "set.seed(3032024) # today\n",
    "\n",
    "# --- First estimate the optimal RF parameters:\n",
    "# the two steps (ntree and mtry) should possibly be iterated\n",
    "dir.create(dirOut)\n",
    "setwd(dirOut)\n",
    "mtry.default=sqrt(dim(xIn)[2]) # default number of variables taken to build the trees\n",
    "if(optimization == 1){\n",
    "  range=\"large\"\n",
    "  Nrand=25\n",
    "  if(range == \"small\"){\n",
    "    ntree.test=seq(from=100,to=500,by=20) # small range\n",
    "  }else{\n",
    "    ntree.test=c(100,250,500, seq(from=1000,to=11000,by=2000)) # large\n",
    "    ntree.test=c(ntree.test,14000,18000,22000) # very large\n",
    "    #ntree.test=c(ntree.test,12000,14000,16000,18000,20000) # very large\n",
    "  }\n",
    "  mtry.test=mtry.default #300 # if different than default this was fixed after one iteration with the next step below\n",
    "  #OOB=vector(mode=\"numeric\",length=length(ntree.test))\n",
    "  OOB=matrix(0,nrow=length(ntree.test),ncol=Nrand)\n",
    "  i=0\n",
    "  for(ntree.tmp in ntree.test){\n",
    "    i=i+1\n",
    "    for(k in 1:Nrand){\n",
    "      RF.tmp <-randomForest(y=yIn,x=xIn,\n",
    "                            importance=T, proximity = T, \n",
    "                            ntree=ntree.tmp,mtry = mtry.test)\n",
    "      \n",
    "      if (type_RF == \"classification\") {\n",
    "        OOB[i,k]=mean(RF.tmp$err.rate[,1])\n",
    "      } else if (type_RF == \"regression\") {\n",
    "        OOB[i,k]=mean(RF.tmp$mse)\n",
    "      } else {\n",
    "        warning(\"type not recognised\")\n",
    "      }\n",
    "      \n",
    "    }\n",
    "  }\n",
    "  OOB.mean=rowMeans(OOB)\n",
    "  OOB.std=apply(OOB,1, sd, na.rm = TRUE)\n",
    "  OOB.df=data.frame(cbind(ntree.test,OOB.mean,OOB.std))\n",
    "  #rownames(OOB.df)=ntree.test\n",
    "  OOB.min.id=which.min(OOB.mean)\n",
    "  ntree.min=OOB.df$ntree.test[OOB.min.id]\n",
    "  ymin=OOB.df$OOB.mean-OOB.df$OOB.std/sqrt(Nrand)\n",
    "  ymax=OOB.df$OOB.mean+OOB.df$OOB.std/sqrt(Nrand)\n",
    "  fileOut=paste(\"optimization_ntree_Class-\",select.class,\"_mtry\",trunc(mtry.test),\n",
    "                \"_\",range,\".csv\",sep=\"\")\n",
    "  write.table(OOB.df,file = fileOut,sep=\"\\t\",quote = FALSE,row.names = FALSE)\n",
    "  plotOut=paste(\"Plot_optimization_ntree_Class-\",select.class,\"_mtry\",trunc(mtry.test),\n",
    "                \"_\",range,\".pdf\",sep=\"\")\n",
    "  pdf(plotOut)\n",
    "  g=ggplot()+\n",
    "    geom_vline(xintercept = ntree.min,linetype = 'dotted', col = 'red')+\n",
    "    geom_point(data=OOB.df,aes(x=ntree.test,y=OOB.mean))+\n",
    "    ylab(\"Mean OOB error\")+xlab(\"Number of trees\")+\n",
    "    scale_y_continuous(trans='log10')+\n",
    "    geom_errorbar(aes(x=ntree.test,ymin=ymin,ymax=ymax))+\n",
    "    theme_bw()\n",
    "  print(g)\n",
    "  dev.off()\n",
    "  \n",
    "  # --- Now we fix the optimal ntree and look for the optimization of mtry\n",
    "  ntree.in=ntree.min\n",
    "  #ntree.in = 6000 # same order of magnitude than the minimum.\n",
    "  mtry.test=seq(from=mtry.default/2,to= dim(xIn)[2],by=mtry.default/2)\n",
    "  #OOB=vector(mode=\"numeric\",length=length(mtry.test))\n",
    "  OOB=matrix(0,nrow=length(mtry.test),ncol=Nrand)\n",
    "  i=0\n",
    "  for(mtry.tmp in mtry.test){\n",
    "    i=i+1\n",
    "    for(k in 1:Nrand){\n",
    "      RF.tmp <-randomForest(y=yIn,x=xIn,\n",
    "                            importance=T, proximity = T, \n",
    "                            ntree=ntree.in,mtry=mtry.tmp)\n",
    "      \n",
    "      if (type_RF == \"classification\") {\n",
    "        OOB[i,k]=mean(RF.tmp$err.rate[,1])\n",
    "      } else if (type_RF == \"regression\") {\n",
    "        OOB[i,k]=mean(RF.tmp$mse)\n",
    "      } else {\n",
    "        warning(\"type not recognised\")\n",
    "      }\n",
    "      \n",
    "    }\n",
    "  }\n",
    "  OOB.mean=rowMeans(OOB)\n",
    "  OOB.std=apply(OOB,1, sd, na.rm = TRUE)\n",
    "  OOB.df=data.frame(cbind(mtry.test,OOB.mean,OOB.std))\n",
    "  #rownames(OOB.df)=ntree.test\n",
    "  OOB.min.id=which.min(OOB.mean)\n",
    "  mtry.min=OOB.df$mtry.test[OOB.min.id]\n",
    "  ymin=OOB.df$OOB.mean-OOB.df$OOB.std/sqrt(Nrand)\n",
    "  ymax=OOB.df$OOB.mean+OOB.df$OOB.std/sqrt(Nrand)\n",
    "  fileOut=paste(\"optimization_mtry_Class-\",select.class,\"_ntree-\",trunc(ntree.min),\n",
    "                \"_\",range,\".csv\",sep=\"\")\n",
    "  write.table(OOB.df,file = fileOut,sep=\"\\t\",quote = FALSE,row.names = FALSE)\n",
    "  plotOut=paste(\"Plot_optimization_mtry_Class-\",select.class,\n",
    "                \"_ntree\",ntree.in,\".pdf\",sep=\"\")\n",
    "  pdf(plotOut)\n",
    "  g=ggplot()+\n",
    "    geom_point(data=OOB.df,aes(x=mtry.test,y=OOB.mean))+\n",
    "    ylab(\"Mean OOB error\")+xlab(\"Number of variables\")+\n",
    "    geom_errorbar(aes(x=mtry.test,ymin=ymin,ymax=ymax))\n",
    "  print(g)\n",
    "  dev.off()\n",
    "}else{ # if we do not optimize\n",
    "  if(mtry.min == \"default\"){ # we need to give a value if the user choose a default value\n",
    "    mtry.min = mtry.default\n",
    "  }\n",
    "}\n",
    "\n",
    "# --- With the optimal parameters run the RF\n",
    "ntree.in=ntree.min\n",
    "mtry.in=mtry.min\n",
    "fileOut=paste(\"RandForestOut_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".RDS\",sep=\"\")\n",
    "if(run_RF == 1){\n",
    "  RF.out <-randomForest(y=yIn,x=xIn,\n",
    "                        importance=T, proximity = T, \n",
    "                        ntree=ntree.in,mtry=mtry.in)\n",
    "  # --- Have a look at the output\n",
    "  RF.out\n",
    "  \n",
    "  if (type_RF == \"classification\") {\n",
    "    mean(RF.out$err.rate[,1]) # this is the mean OOB\n",
    "  } else if (type_RF == \"regression\") {\n",
    "    mean(RF.out$mse) # this is the mean OOB\n",
    "  } else {\n",
    "    warning(\"type not recognised\")\n",
    "  }\n",
    "  \n",
    "  saveRDS(RF.out, file = fileOut)\n",
    "}else{\n",
    "  RF.out = readRDS(file = fileOut)\n",
    "}\n",
    "\n",
    "RF.out \n",
    "\n",
    "# ANALYSE -------------\n",
    "\n",
    "# --- Extract important variables\n",
    "setwd(dirOut)\n",
    "importance.df = measure_importance(RF.out)\n",
    "ASV.top = important_variables(importance.df)\n",
    "\n",
    "fileOut=paste(\"varImpExt_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".tsv\",sep=\"\")\n",
    "\n",
    "write.table(importance.df,file=fileOut,sep = \"\\t\",quote=FALSE)\n",
    "\n",
    "fileOut=paste(\"varTop_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".tsv\",sep=\"\")\n",
    "\n",
    "write.table(ASV.top,file=fileOut,sep = \"\\t\",quote=FALSE)\n",
    "\n",
    "# Plots ---------\n",
    "\n",
    "# first and overview, it will generate an html file. Takes time.\n",
    "explain_forest(RF.out)\n",
    "\n",
    "# --- Error\n",
    "plotOut=paste(\"Plot_Error_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".pdf\",sep=\"\")\n",
    "pdf(plotOut)\n",
    "plot(RF.out)\n",
    "dev.off()\n",
    "\n",
    "\n",
    "# --- Variable importance\n",
    "fileOut=paste(\"varImp_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".tsv\",sep=\"\")\n",
    "varImp.out=varImp(RF.out)\n",
    "varImp.out=varImp.out[which(rowSums(varImp.out) != 0),]\n",
    "write.table(varImp.out,file=fileOut,sep = \"\\t\",quote=FALSE)\n",
    "plotOut=paste(\"Plot_varImp_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".pdf\",sep=\"\")\n",
    "pdf(plotOut,width=10)\n",
    "#varImpPlot(RF.out, type=1,main=\"\")#,xlab=\"Variable importance\",title=\"\")\n",
    "#dev.off()\n",
    "\n",
    "# Get variable importance from the model fit\n",
    "ImpData <- as.data.frame(importance(RF.out))\n",
    "ImpData$Var.Names <- row.names(ImpData)\n",
    "ImpData.sort.idx = sort(ImpData$MeanDecreaseAccuracy, \n",
    "                        decreasing = TRUE, index.return = T)\n",
    "quantile(ImpData.sort.idx$x)\n",
    "Imp.Data.sort = ImpData[ImpData.sort.idx$ix, ]\n",
    "explained = 50\n",
    "id.exp = which(Imp.Data.sort$MeanDecreaseAccuracy > explained)\n",
    "Imp.Data.sort = Imp.Data.sort[id.exp,]\n",
    "\n",
    "ggplot(Imp.Data.sort, aes(x=Var.Names, y=MeanDecreaseAccuracy)) +\n",
    "  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=MeanDecreaseAccuracy), color=\"skyblue\") +\n",
    "  geom_point(aes(size = MeanDecreaseGini), color=\"blue\", alpha=0.6) +\n",
    "  theme_light() +\n",
    "  coord_flip() +\n",
    "  theme(\n",
    "    legend.position=\"bottom\",\n",
    "    panel.grid.major.y = element_blank(),\n",
    "    panel.border = element_blank(),\n",
    "    axis.ticks.y = element_blank()\n",
    "  )\n",
    "\n",
    "dev.off()\n",
    "\n",
    "# Variable importance, multiway\n",
    "\n",
    "plotOut=paste(\"Plot_accuracyVsGini_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".pdf\",sep=\"\")\n",
    "\n",
    "pdf(plotOut, height =6)\n",
    "p = plot_multi_way_importance(\n",
    "  importance_frame = importance.df,\n",
    "  x_measure = \"accuracy_decrease\",\n",
    "  y_measure = \"gini_decrease\",\n",
    "  size_measure = \"p_value\",\n",
    "  min_no_of_trees = 0,\n",
    "  no_of_labels = 10 #,\n",
    "  #main = \"Multi-way importance plot\"\n",
    ")\n",
    "p = p + xlab(\"Accuracy decrease\") +ylab(\"Gini index decrease\")\n",
    "p\n",
    "#print(p)\n",
    "dev.off()\n",
    "\n",
    "# --- Partial plots sorted by importance\n",
    "# ... these plots may take a long time\n",
    "if(partial.plots == 1){\n",
    "  Nsel=10 # select only 10 vars\n",
    "  imp <- importance(RF.out)\n",
    "  impvar <- rownames(imp)[order(imp[, 1], decreasing=TRUE)]\n",
    "  \n",
    "  impvar=impvar[1:Nsel]\n",
    "  Nlev=levels(yIn)\n",
    "  \n",
    "  # Interpret partial plots\n",
    "  # https://stats.stackexchange.com/questions/147763/meaning-of-y-axis-in-random-forest-partial-dependence-plot\n",
    "  # p=seq(from=0.005, to=0.995, by=0.005) # to understand the plots\n",
    "  # plot(p,log(p/(1-p))) # plot the logit function\n",
    "  for (i in seq_along(impvar)) {\n",
    "    var.lab=str_replace_all(impvar[i],pattern=\"[[:punct:]]\",replacement = \"\")\n",
    "    var.lab=str_replace_all(var.lab,pattern = \" \",replacement = \"_\")\n",
    "    plotOut=paste(\"Plot_PartialDependence_\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "                  \"Class-\",select.class,\"_Var-\",var.lab,\n",
    "                  \".pdf\",sep=\"\")\n",
    "    pdf(file=plotOut,width=12)\n",
    "    op <- par(mfrow=c(2, 3))\n",
    "    for(level in levels(yIn)){\n",
    "      partialPlot(RF.out, xIn, impvar[i], xlab=impvar[i],\n",
    "                  which.class = level,\n",
    "                  main=paste(\"Partial Dependence for class\", level)) #ylim=c(30, 70))\n",
    "    }\n",
    "    par(op)\n",
    "    dev.off()\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "# \n",
    "# \n",
    "# data(airquality)\n",
    "# airquality <- na.omit(airquality)\n",
    "# set.seed(131)\n",
    "# ozone.rf <- randomForest(Ozone ~ ., airquality, importance=TRUE)\n",
    "# imp <- importance(ozone.rf)\n",
    "# impvar <- rownames(imp)[order(imp[, 1], decreasing=TRUE)]\n",
    "# op <- par(mfrow=c(2, 3))\n",
    "# for (i in seq_along(impvar)) {\n",
    "#   partialPlot(ozone.rf, airquality, impvar[i], xlab=impvar[i],\n",
    "#               main=paste(\"Partial Dependence on\", impvar[i]),\n",
    "#               ylim=c(30, 70))\n",
    "# }\n",
    "# par(op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a23120-064b-4f71-923e-020a7a7a1551",
   "metadata": {},
   "source": [
    "# BREAKAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf8224-9236-4f5e-9ec3-3670a07f9422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################# RANDOM FOREST computation -------------\n",
    "set.seed(3032024) # today\n",
    "\n",
    "# --- First estimate the optimal RF parameters:\n",
    "# the two steps (ntree and mtry) should possibly be iterated\n",
    "dir.create(dirOut)\n",
    "setwd(dirOut)\n",
    "mtry.default=sqrt(dim(xIn)[2]) # default number of variables taken to build the trees\n",
    "if(optimization == 1){\n",
    "  range=\"large\"\n",
    "  Nrand=25\n",
    "  if(range == \"small\"){\n",
    "    ntree.test=seq(from=100,to=500,by=20) # small range\n",
    "  }else{\n",
    "    ntree.test=c(100,250,500, seq(from=1000,to=11000,by=2000)) # large\n",
    "    ntree.test=c(ntree.test,14000,18000,22000) # very large\n",
    "    #ntree.test=c(ntree.test,12000,14000,16000,18000,20000) # very large\n",
    "  }\n",
    "  mtry.test=mtry.default #300 # if different than default this was fixed after one iteration with the next step below\n",
    "  #OOB=vector(mode=\"numeric\",length=length(ntree.test))\n",
    "  OOB=matrix(0,nrow=length(ntree.test),ncol=Nrand)\n",
    "  i=0\n",
    "  for(ntree.tmp in ntree.test){\n",
    "    i=i+1\n",
    "    for(k in 1:Nrand){\n",
    "      RF.tmp <-randomForest(y=yIn,x=xIn,\n",
    "                            importance=T, proximity = T, \n",
    "                            ntree=ntree.tmp,mtry = mtry.test)\n",
    "      \n",
    "      if (type_RF == \"classification\") {\n",
    "        OOB[i,k]=mean(RF.tmp$err.rate[,1])\n",
    "      } else if (type_RF == \"regression\") {\n",
    "        OOB[i,k]=mean(RF.tmp$mse)\n",
    "      } else {\n",
    "        warning(\"type not recognised\")\n",
    "      }\n",
    "      \n",
    "    }\n",
    "  }\n",
    "  OOB.mean=rowMeans(OOB)\n",
    "  OOB.std=apply(OOB,1, sd, na.rm = TRUE)\n",
    "  OOB.df=data.frame(cbind(ntree.test,OOB.mean,OOB.std))\n",
    "  #rownames(OOB.df)=ntree.test\n",
    "  OOB.min.id=which.min(OOB.mean)\n",
    "  ntree.min=OOB.df$ntree.test[OOB.min.id]\n",
    "  ymin=OOB.df$OOB.mean-OOB.df$OOB.std/sqrt(Nrand)\n",
    "  ymax=OOB.df$OOB.mean+OOB.df$OOB.std/sqrt(Nrand)\n",
    "  fileOut=paste(\"optimization_ntree_Class-\",select.class,\"_mtry\",trunc(mtry.test),\n",
    "                \"_\",range,\".csv\",sep=\"\")\n",
    "  write.table(OOB.df,file = fileOut,sep=\"\\t\",quote = FALSE,row.names = FALSE)\n",
    "  plotOut=paste(\"Plot_optimization_ntree_Class-\",select.class,\"_mtry\",trunc(mtry.test),\n",
    "                \"_\",range,\".pdf\",sep=\"\")\n",
    "  pdf(plotOut)\n",
    "  g=ggplot()+\n",
    "    geom_vline(xintercept = ntree.min,linetype = 'dotted', col = 'red')+\n",
    "    geom_point(data=OOB.df,aes(x=ntree.test,y=OOB.mean))+\n",
    "    ylab(\"Mean OOB error\")+xlab(\"Number of trees\")+\n",
    "    scale_y_continuous(trans='log10')+\n",
    "    geom_errorbar(aes(x=ntree.test,ymin=ymin,ymax=ymax))+\n",
    "    theme_bw()\n",
    "  print(g)\n",
    "  dev.off()\n",
    "  \n",
    "  # --- Now we fix the optimal ntree and look for the optimization of mtry\n",
    "  ntree.in=ntree.min\n",
    "  #ntree.in = 6000 # same order of magnitude than the minimum.\n",
    "  mtry.test=seq(from=mtry.default/2,to= dim(xIn)[2],by=mtry.default/2)\n",
    "  #OOB=vector(mode=\"numeric\",length=length(mtry.test))\n",
    "  OOB=matrix(0,nrow=length(mtry.test),ncol=Nrand)\n",
    "  i=0\n",
    "  for(mtry.tmp in mtry.test){\n",
    "    i=i+1\n",
    "    for(k in 1:Nrand){\n",
    "      RF.tmp <-randomForest(y=yIn,x=xIn,\n",
    "                            importance=T, proximity = T, \n",
    "                            ntree=ntree.in,mtry=mtry.tmp)\n",
    "      \n",
    "      if (type_RF == \"classification\") {\n",
    "        OOB[i,k]=mean(RF.tmp$err.rate[,1])\n",
    "      } else if (type_RF == \"regression\") {\n",
    "        OOB[i,k]=mean(RF.tmp$mse)\n",
    "      } else {\n",
    "        warning(\"type not recognised\")\n",
    "      }\n",
    "      \n",
    "    }\n",
    "  }\n",
    "  OOB.mean=rowMeans(OOB)\n",
    "  OOB.std=apply(OOB,1, sd, na.rm = TRUE)\n",
    "  OOB.df=data.frame(cbind(mtry.test,OOB.mean,OOB.std))\n",
    "  #rownames(OOB.df)=ntree.test\n",
    "  OOB.min.id=which.min(OOB.mean)\n",
    "  mtry.min=OOB.df$mtry.test[OOB.min.id]\n",
    "  ymin=OOB.df$OOB.mean-OOB.df$OOB.std/sqrt(Nrand)\n",
    "  ymax=OOB.df$OOB.mean+OOB.df$OOB.std/sqrt(Nrand)\n",
    "  fileOut=paste(\"optimization_mtry_Class-\",select.class,\"_ntree-\",trunc(ntree.min),\n",
    "                \"_\",range,\".csv\",sep=\"\")\n",
    "  write.table(OOB.df,file = fileOut,sep=\"\\t\",quote = FALSE,row.names = FALSE)\n",
    "  plotOut=paste(\"Plot_optimization_mtry_Class-\",select.class,\n",
    "                \"_ntree\",ntree.in,\".pdf\",sep=\"\")\n",
    "  pdf(plotOut)\n",
    "  g=ggplot()+\n",
    "    geom_point(data=OOB.df,aes(x=mtry.test,y=OOB.mean))+\n",
    "    ylab(\"Mean OOB error\")+xlab(\"Number of variables\")+\n",
    "    geom_errorbar(aes(x=mtry.test,ymin=ymin,ymax=ymax))\n",
    "  print(g)\n",
    "  dev.off()\n",
    "}else{ # if we do not optimize\n",
    "  if(mtry.min == \"default\"){ # we need to give a value if the user choose a default value\n",
    "    mtry.min = mtry.default\n",
    "  }\n",
    "}\n",
    "\n",
    "# --- With the optimal parameters run the RF\n",
    "ntree.in=ntree.min\n",
    "mtry.in=mtry.min\n",
    "fileOut=paste(\"RandForestOut_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".RDS\",sep=\"\")\n",
    "if(run_RF == 1){\n",
    "  RF.out <-randomForest(y=yIn,x=xIn,\n",
    "                        importance=T, proximity = T, \n",
    "                        ntree=ntree.in,mtry=mtry.in)\n",
    "  # --- Have a look at the output\n",
    "  RF.out\n",
    "  \n",
    "  if (type_RF == \"classification\") {\n",
    "    mean(RF.out$err.rate[,1]) # this is the mean OOB\n",
    "  } else if (type_RF == \"regression\") {\n",
    "    mean(RF.out$mse) # this is the mean OOB\n",
    "  } else {\n",
    "    warning(\"type not recognised\")\n",
    "  }\n",
    "  \n",
    "  saveRDS(RF.out, file = fileOut)\n",
    "}else{\n",
    "  RF.out = readRDS(file = fileOut)\n",
    "}\n",
    "\n",
    "RF.out \n",
    "\n",
    "################################# ANALYSE -------------\n",
    "\n",
    "# --- Extract important variables\n",
    "setwd(dirOut)\n",
    "importance.df = measure_importance(RF.out)\n",
    "ASV.top = important_variables(importance.df)\n",
    "\n",
    "fileOut=paste(\"varImpExt_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".tsv\",sep=\"\")\n",
    "\n",
    "write.table(importance.df,file=fileOut,sep = \"\\t\",quote=FALSE)\n",
    "\n",
    "fileOut=paste(\"varTop_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".tsv\",sep=\"\")\n",
    "\n",
    "write.table(ASV.top,file=fileOut,sep = \"\\t\",quote=FALSE)\n",
    "\n",
    "# Plots ---------\n",
    "\n",
    "# first and overview, it will generate an html file. Takes time.\n",
    "explain_forest(RF.out)\n",
    "\n",
    "# --- Error\n",
    "plotOut=paste(\"Plot_Error_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".pdf\",sep=\"\")\n",
    "pdf(plotOut)\n",
    "plot(RF.out)\n",
    "dev.off()\n",
    "\n",
    "\n",
    "# --- Variable importance\n",
    "fileOut=paste(\"varImp_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".tsv\",sep=\"\")\n",
    "varImp.out=varImp(RF.out)\n",
    "varImp.out=varImp.out[which(rowSums(varImp.out) != 0),]\n",
    "write.table(varImp.out,file=fileOut,sep = \"\\t\",quote=FALSE)\n",
    "plotOut=paste(\"Plot_varImp_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".pdf\",sep=\"\")\n",
    "pdf(plotOut,width=10)\n",
    "#varImpPlot(RF.out, type=1,main=\"\")#,xlab=\"Variable importance\",title=\"\")\n",
    "#dev.off()\n",
    "\n",
    "# Get variable importance from the model fit\n",
    "ImpData <- as.data.frame(importance(RF.out))\n",
    "ImpData$Var.Names <- row.names(ImpData)\n",
    "ImpData.sort.idx = sort(ImpData$MeanDecreaseAccuracy, \n",
    "                        decreasing = TRUE, index.return = T)\n",
    "quantile(ImpData.sort.idx$x)\n",
    "Imp.Data.sort = ImpData[ImpData.sort.idx$ix, ]\n",
    "explained = 50\n",
    "id.exp = which(Imp.Data.sort$MeanDecreaseAccuracy > explained)\n",
    "Imp.Data.sort = Imp.Data.sort[id.exp,]\n",
    "\n",
    "ggplot(Imp.Data.sort, aes(x=Var.Names, y=MeanDecreaseAccuracy)) +\n",
    "  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=MeanDecreaseAccuracy), color=\"skyblue\") +\n",
    "  geom_point(aes(size = MeanDecreaseGini), color=\"blue\", alpha=0.6) +\n",
    "  theme_light() +\n",
    "  coord_flip() +\n",
    "  theme(\n",
    "    legend.position=\"bottom\",\n",
    "    panel.grid.major.y = element_blank(),\n",
    "    panel.border = element_blank(),\n",
    "    axis.ticks.y = element_blank()\n",
    "  )\n",
    "\n",
    "dev.off()\n",
    "\n",
    "# Variable importance, multiway\n",
    "\n",
    "plotOut=paste(\"Plot_accuracyVsGini_Class-\",select.class,\n",
    "              \"_ntree\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "              \".pdf\",sep=\"\")\n",
    "\n",
    "pdf(plotOut, height =6)\n",
    "p = plot_multi_way_importance(\n",
    "  importance_frame = importance.df,\n",
    "  x_measure = \"accuracy_decrease\",\n",
    "  y_measure = \"gini_decrease\",\n",
    "  size_measure = \"p_value\",\n",
    "  min_no_of_trees = 0,\n",
    "  no_of_labels = 10 #,\n",
    "  #main = \"Multi-way importance plot\"\n",
    ")\n",
    "p = p + xlab(\"Accuracy decrease\") +ylab(\"Gini index decrease\")\n",
    "p\n",
    "#print(p)\n",
    "dev.off()\n",
    "\n",
    "# --- Partial plots sorted by importance\n",
    "# ... these plots may take a long time\n",
    "if(partial.plots == 1){\n",
    "  Nsel=10 # select only 10 vars\n",
    "  imp <- importance(RF.out)\n",
    "  impvar <- rownames(imp)[order(imp[, 1], decreasing=TRUE)]\n",
    "  \n",
    "  impvar=impvar[1:Nsel]\n",
    "  Nlev=levels(yIn)\n",
    "  \n",
    "  # Interpret partial plots\n",
    "  # https://stats.stackexchange.com/questions/147763/meaning-of-y-axis-in-random-forest-partial-dependence-plot\n",
    "  # p=seq(from=0.005, to=0.995, by=0.005) # to understand the plots\n",
    "  # plot(p,log(p/(1-p))) # plot the logit function\n",
    "  for (i in seq_along(impvar)) {\n",
    "    var.lab=str_replace_all(impvar[i],pattern=\"[[:punct:]]\",replacement = \"\")\n",
    "    var.lab=str_replace_all(var.lab,pattern = \" \",replacement = \"_\")\n",
    "    plotOut=paste(\"Plot_PartialDependence_\",trunc(ntree.in),\"_mtry\",trunc(mtry.in),\n",
    "                  \"Class-\",select.class,\"_Var-\",var.lab,\n",
    "                  \".pdf\",sep=\"\")\n",
    "    pdf(file=plotOut,width=12)\n",
    "    op <- par(mfrow=c(2, 3))\n",
    "    for(level in levels(yIn)){\n",
    "      partialPlot(RF.out, xIn, impvar[i], xlab=impvar[i],\n",
    "                  which.class = level,\n",
    "                  main=paste(\"Partial Dependence for class\", level)) #ylim=c(30, 70))\n",
    "    }\n",
    "    par(op)\n",
    "    dev.off()\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f01f6-3d02-42aa-a622-ea95aed39543",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Same as Exercise 1 but now each predictor is one of your clusters in the network, and you will take \n",
    "the sum of the total abundance of its members. You will also include the logarithm of the total \n",
    "abundance of each sample.\n",
    "Compare both exercises, are results consistent? Does the prediction improve by using the clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef441556-f1a5-443c-b944-e6d1db5930aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at distribution of clusters\n",
    "# remove clusters with only 1 asv\n",
    "\n",
    "# Remove either location or partition\n",
    "# BUt first compare include v not include these bariables by comparing MSE\n",
    "\n",
    "# Plot mean relative abundance of interesting clusters across startnign communities and final communities (zx, y) and regression\n",
    "\n",
    "# Use ALberto's document for RF\n",
    "\n",
    "# RF using one final replciate as trainign and then predict function of other 3 replicated\n",
    "# THen second RF where combine all replicates as before\n",
    "# DO for both ASV and cluster\n",
    "\n",
    "# Compare using presence/absence for RF to predict function with using rel abundance with using cluster\n",
    "\n",
    "# Narrative of paper\n",
    "# Genotype to phenotype\n",
    "# Structure to function\n",
    "# COmparing different ways of quantifying structure (composition) to predicting different functions\n",
    "\n",
    "# REad Statistically learning the functional landscape of microbial communities to come up with narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de882f2c-abfb-44aa-9259-42d6fc1cf6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
