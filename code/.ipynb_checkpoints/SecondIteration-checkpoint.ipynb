{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf7df8b-5b50-45ba-8362-88d1c1960ca2",
   "metadata": {},
   "source": [
    "Summary of 'Replaying the tape of ecology to domesticate wild microbiota'\n",
    "=====\n",
    "My research project uses data from 'Replaying the tape of ecology to domesticate wild microbiota'. Below is a summary of this paper:\n",
    "\n",
    "#### Research questions answered:\n",
    "    - Can bacterial community assembly be reproduced?\n",
    "    - Do reproduced bacterial communities confer the same function?\n",
    "      \n",
    "#### Experiment:\n",
    "    - 275 starting communities, each from a unique tree hole\n",
    "    - Different initial taxonomic compositions\n",
    "    - Composition of these starting communities before cryopreservation\n",
    "    - After cryopreservation, 4 replicates for each of 275 starting communities, standardised environment for 7 days, then sample composition\n",
    "    - Communities associated with leaf litter degradation function\n",
    "\n",
    "#### Techniques used:\n",
    "    - Identified that the 4 final community replicates of each of the 275 tree holes were clustered\n",
    "    - Used ordination to track changes in abundance of taxa, showing that cirection of travel from starting to final communities consistent among communities and replicates\n",
    "    - Unsupervised clustering of communities based on composition - 5 initial classes, 2 final classes\n",
    "    - Metagenomic predictions of functional differences between classes: identify associations between 2 final community classes and functional performance of communities by inputting metagenomes of communities from 16S rRNA sequencing data, using PiCRUST to categorise genes by function using the KEGG database - find out what aorts of genes the community classes have in terms of what the genes do ex. take up nutrients\n",
    "    - Direct measurements of functional performance of final communities: degradation rate of 4 substrates, whole community metabolic activity, respiration rate, and cell numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38893b86-d209-405b-8aec-e804ca17d692",
   "metadata": {},
   "source": [
    "My biological research question:\n",
    "======\n",
    "### How do differences in the network structure of microbial communities link to function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb104275-3da3-40c3-88ab-4697039d6b0c",
   "metadata": {},
   "source": [
    "How can this be achieved?\n",
    "=====\n",
    "\n",
    "#### Co-occurrence network options\n",
    "\n",
    "Co-occurrence network of starting communities:\n",
    "There are 2 options...\n",
    "  \n",
    "- OPTION 1: Because there are no replicates for the starting communities of a given tree hole, only one co-occurrence network can be constructed across the starting communities of all of the tree holes.\n",
    "\n",
    "- OPTION 2: Separate the tree holes according to community class, and construct co-occurrence networks for each class (5 in total).\n",
    "\n",
    "Co-occurrence network of final communities:\n",
    "There are 3 options...\n",
    "  \n",
    "- OPTION 1: Calculate a co-occurrence network across all tree holes for each final community replicate. This would result in 4 co-occurrence networks. The co-occurrence network for final community replicate 1, for instance, would be across the replicate 1 final communities for all 275 tree holes. Could also do the same, whilst ignoring replicates, resulting in a network across 4 replicates of the 275 tree holes (1100 samples) - this seems a bit strange as the number of samples between this network and the starting network are mismatched.\n",
    "  \n",
    "- OPTION 2: Because there are 4 replicate final communities for every tree hole (and thus every starting community), it is possible to create a co-occurrence network for each tree hole across its 4 replicates. This would result in 275 co-occurrence networks. Unlike in the starting community co-occurrence network and in OPTION 1, here we would be calculating co-occurrence across replicates of a single tree hole, rather than co-occurrence across multiple tree holes. I am not sure how comparable this would be to the starting network, given that the starting tree holes are heterogeneous, so they seemingly cannot be generalised to replicates.\n",
    "  \n",
    "- OPTION 3: Separate the tree holes according to community class and replicate, and construct a co-occurrence network for each class within each replicate (8 total because 2 community classes in each of 4 replicates). \n",
    "\n",
    "#### Narrowing down the options\n",
    "- I think that OPTION 2 for the starting community co-occurrence network and OPTION 3 for the final community co-occurrence network are the best options. Because the function of the community classes are known, the functions of the networks constructed within the community classes will also be known. The structure of the networks in the different classes can then be compared in relation to this function\n",
    "- PROBLEM: How does this add to the previous paper? What can we use to compare network structure in a biologically meaningful way. I am confused about what additional information constructing a network adds to the existing information upon community composition that is available for each community class.\n",
    "\n",
    "- ALTERNATIVE:\n",
    "- Could use final community OPTION 2 to get a co-occurrence network for each tree hole, and then compare these co-occurrence network structures in relation to the known function of each sample. Here, we would either ignore the starting communities altogether, or treat them as a single tree hole - whereby each individual tree hole starting communtiy would be treated as a replicate as an overall meta-treehole.\n",
    "- PROBLEM: The starting tree hole communities were different, so there wasn't really a single starting poitn for all of the final communities\n",
    "- PROBLEM: Still need to find out what network features to comaprei n a biologically meaningful way, and how this adds to the previosu paper.\n",
    "\n",
    "#### Current choice:\n",
    "- I have currently chosen OPTION 2 for the starting community co-occurrence network and OPTION 3 for the final community co-occurrence network - I will split the data by samples and replicate, and construct co-occurrence network for each combination.\n",
    "\n",
    "#### What to compare between networks for biological meaning\n",
    "- Main question is what to compare between networks for biological meaning. We already know the composition of each community class, and so we already know the composition of each network (assuming we pick the combination of starting community OPTION 2 and final communtiy OPTION 3). What can we compare that provides additional information. Here are some options:\n",
    "    - Number of links\n",
    "    - Topology\n",
    "    - Central species\n",
    "    - Presence and types of communities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd5d620-009f-464c-8230-8484626938ed",
   "metadata": {},
   "source": [
    "Pipeline\n",
    "======\n",
    "This is the pipeline for Matthew Shaun Grainger's MRes Research Project.\n",
    "Overall, it involves constructing co-occurrence networks for each class of each replicate of the starting and final microbial communities, and then comparing the structures of these networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fb9ef7-e057-4743-a4db-1ab8895434ce",
   "metadata": {},
   "source": [
    "## 1. Preliminaries\n",
    "1. Importing all of the data from the treeholes paper\n",
    "2. Familiarizing myself with the data, in particular with the metadata. Extracting the communities that I will be working with.\n",
    "3. Installing FlashWeave, and making it work. So far, it seems to outperform other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84405c12-f988-49f9-8b65-019a428aceae",
   "metadata": {},
   "source": [
    "#### 1) Importing all of the data from the treeholes paper\n",
    "The below code is written in Python - be sure to switch to the Python kernel to run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de59dcaf-c7fc-402c-a0c7-17377491d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import pandas as pd\n",
    "\n",
    "# Importing ASV table\n",
    "asv_table = pd.read_csv('../data/seqtable_readyforanalysis.csv', sep='\\t')\n",
    "# Importing metadata\n",
    "metadata = pd.read_csv('../data/metadata_Time0D-7D-4M_May2022_wJSDpart_ext.csv', sep='\\t')\n",
    "# Importing taxonomy\n",
    "taxonomy_data = pd.read_csv('../data/taxa_wsp_readyforanalysis.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a658d1-1397-4329-bad4-30579cfcde29",
   "metadata": {},
   "source": [
    "#### 2) Familiarizing myself with the data, in particular with the metadata. Extracting the communities that I will be working with.\n",
    "The below code is written in Python - be sure to switch to the Python kernel to run it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671338f-ad13-4326-8dd9-51d7e611c8ae",
   "metadata": {},
   "source": [
    "##### ASV Table\n",
    "The ASV table is in the standard format. There is a column for each ASV, and there is a row for each sample. Each cell contains the abundance of an OTU in a given sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97525294-fefe-4b98-b7ff-29218df71fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the ASV table\n",
    "asv_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0e90c4-b0aa-4240-9128-0b7d52865020",
   "metadata": {},
   "source": [
    "##### Metadata\n",
    "The metadata contains a row for each sample. It has columns which provide additional information about each sample. Out of these columns, the following are relevant:\n",
    "- sampleid: Each sample has an unique ID\n",
    "- replicate: Replicate of the experiment. Starting communities are labelled as Rep0\n",
    "- parent: ID of the starting communities from which final communities departed. For example, final community with id WYT14.1 is the replicate 1 of starting community WYT14, which is the ID indicated in this field.\n",
    "- Location: Sampling field from which the starting communities were sampled.\n",
    "- Experiment: Either starting communities (0D) or final communities (7D_rep$) were \"$\" = 1-4 depending on the replicate. Samples belonging to experiment 4M should be ignored.\n",
    "- Part_Time0D_17: Id of the class the starting communities belong to, corresponding to the maximum of the Calinski-Harabasz index found considering starting communities only. (1 to 17 and NA for samples not belonging to the set)\n",
    "- Part_Time0D_6: Id of the class the starting communities belong to, corresponding to the second maximum of the Calinski-Harabasz index found considering starting communities only (analysed in this work) (runs from 1 to 6 and NA for samples not belonging to the set)\n",
    "- Part_Time7D_rep1_2: Id of the class the first replicate of final communities belong to, corresponding to the maximum of the Calinski-Harabasz index. (1 to 2 and NA for samples not belonging to the set)\n",
    "- Part_Time7D_rep2_2: Id of the class the second replicate of final communities belong to, corresponding to the maximum of the Calinski-Harabasz index. (1 to 2 and NA for samples not belonging to the set)\n",
    "- Part_Time7D_rep3_2: Id of the class the third replicate of final communities belong to, corresponding to the maximum of the Calinski-Harabasz index. (1 to 2 and NA for samples not belonging to the set)\n",
    "- Part_Time7D_rep4_2: Id of the class the fourth replicate of final communities belong to, corresponding to the maximum of the Calinski-Harabasz index. (1 to 2 and NA for samples not belonging to the set)\n",
    "- replicate.partition: Combination of the replicate and partition ids. Note that the ids obtained for each replicate independently (1 and 2 for each replicate) can be considered paired across replicates, since we showed they have similar compositions. Also note that Rep0.Class1 and Rep0.Class2 are ids used in Experiments 0D and 4M. Therefore, those belonging to Experiment = 4M should be ignored.\n",
    "- partition: Only the class. As in the previous field, one should exclude samples in Experiment = 4M.\n",
    "- ExpCompact: Another identifier for the experiment, in which the 4 replicates of final communities have the same id (note that in the field Experiment the different replicates were differentiated). Levels are Starting, Final (and Evolved should be excluded).\n",
    "- exp.replicate.partition: Combination of ExpCompact, and replicate.partition\n",
    "- exp.partition: Combination of ExpCompact, and partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e18df28-9f9b-4b42-9129-1199723941b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865cb50-8fab-4b01-bee9-3625a4ac6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names of the columns in the metadata\n",
    "print(metadata.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730bf14c-73b7-4700-89d1-492699d22cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata['replicate.partition'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2390305-61c4-46a8-8955-dd16bc034adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata['exp.replicate.partition'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2c436d-21e6-45df-b7c7-bc25282b2a7a",
   "metadata": {},
   "source": [
    "Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad21ec-db0e-4aba-b03a-778bd1c3c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1982157-ec93-4ba9-ab37-b6053588ad11",
   "metadata": {},
   "source": [
    "#### Extracting the communities that I will be working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd47314b-d18c-4c6e-87b0-d6ec8a05d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of the samples belonging to experiment 4M:\n",
    "asv_table.reset_index(inplace=True) # Making the sample ID into a column for the ASV table\n",
    "asv_table.rename(columns={'index': 'sampleid'}, inplace=True) # renaming this new column to 'sampleid'\n",
    "metadata_asv_table = pd.merge(asv_table, metadata, on='sampleid') # Merging metadata and asv_table by 'sampleid'\n",
    "main_data = metadata_asv_table[metadata_asv_table['Experiment'] != '4M'] # Taking rows which are not 4M samples\n",
    "\n",
    "# Separating the samples by class and replicate (a separate data frame for each class within each replicate).\n",
    "starting_class1 = main_data[main_data['exp.replicate.partition'] == 'Starting.Rep0.Class1']\n",
    "starting_class2 = main_data[main_data['exp.replicate.partition'] == 'Starting.Rep0.Class2']\n",
    "starting_class3 = main_data[main_data['exp.replicate.partition'] == 'Starting.Rep0.Class3']\n",
    "starting_class4 = main_data[main_data['exp.replicate.partition'] == 'Starting.Rep0.Class4']\n",
    "starting_class5 = main_data[main_data['exp.replicate.partition'] == 'Starting.Rep0.Class5']\n",
    "starting_class6 = main_data[main_data['exp.replicate.partition'] == 'Starting.Rep0.Class6']\n",
    "\n",
    "final1_class1 = main_data[main_data['exp.replicate.partition'] == 'Final.Rep1.Class1']\n",
    "final1_class2 = main_data[main_data['exp.replicate.partition'] == 'Final.Rep1.Class2']\n",
    "final2_class1 = main_data[main_data['exp.replicate.partition'] == 'Final.Rep2.Class1']\n",
    "final2_class2 = main_data[main_data['exp.replicate.partition'] == 'Final.Rep2.Class2']\n",
    "final3_class1 = main_data[main_data['exp.replicate.partition'] == 'Final.Rep3.Class1']\n",
    "final3_class2 = main_data[main_data['exp.replicate.partition'] == 'Final.Rep3.Class2']\n",
    "final4_class1 = main_data[main_data['exp.replicate.partition'] == 'Final.Rep4.Class1']\n",
    "final4_class2 = main_data[main_data['exp.replicate.partition'] == 'Final.Rep4.Class2']\n",
    "\n",
    "# Removing metadata columns\n",
    "columns_to_drop = ['sampleid', 'Name.2', 'Community', 'Species', 'replicate',\n",
    "       'BreakingBag', 'parent', 'Location', 'Experiment', 'Part_Time0D_17',\n",
    "'Community', 'Species', 'replicate',\n",
    "       'BreakingBag', 'parent', 'Location', 'Experiment', 'Part_Time0D_17',\n",
    "       'Part_Time0D_6', 'Part_Time4M_64', 'Part_Time7D_rep1_2',\n",
    "       'Part_Time7D_rep2_2', 'Part_Time7D_rep3_2', 'Part_Time7D_rep4_2',\n",
    "       'replicate.partition', 'partition', 'ExpCompact',\n",
    "       'exp.replicate.partition', 'exp.partition', 'Part_Time0D_6', 'Part_Time4M_64', 'Part_Time7D_rep1_2',\n",
    "       'Part_Time7D_rep2_2', 'Part_Time7D_rep3_2', 'Part_Time7D_rep4_2',\n",
    "       'replicate.partition', 'partition', 'ExpCompact',\n",
    "       'exp.replicate.partition', 'exp.partition']\n",
    "\n",
    "starting_class1 = starting_class1.drop(columns=columns_to_drop)\n",
    "starting_class2 = starting_class2.drop(columns=columns_to_drop)\n",
    "starting_class3 = starting_class3.drop(columns=columns_to_drop)\n",
    "starting_class4 = starting_class4.drop(columns=columns_to_drop)\n",
    "starting_class5 = starting_class5.drop(columns=columns_to_drop)\n",
    "starting_class6 = starting_class6.drop(columns=columns_to_drop)\n",
    "\n",
    "final1_class1 = final1_class1.drop(columns=columns_to_drop)\n",
    "final1_class2 = final1_class2.drop(columns=columns_to_drop)\n",
    "final2_class1 = final2_class1.drop(columns=columns_to_drop)\n",
    "final2_class2 = final2_class2.drop(columns=columns_to_drop)\n",
    "final3_class1 = final3_class1.drop(columns=columns_to_drop)\n",
    "final3_class2 = final3_class2.drop(columns=columns_to_drop)\n",
    "final4_class1 = final4_class1.drop(columns=columns_to_drop)\n",
    "final4_class2 = final4_class2.drop(columns=columns_to_drop)\n",
    "\n",
    "# Creating .csv files for each of these subsets.\n",
    "# This code is not intended to be reproducible, so it does not need to be super efficient.\n",
    "# The easiest way to acccess these Python-created subsets in Julia, where Flashweave is coded, is to import them from .csv files.\n",
    "starting_class1.to_csv('../data/starting_class1.csv', index=False)\n",
    "starting_class2.to_csv('../data/starting_class2.csv', index=False)\n",
    "starting_class3.to_csv('../data/starting_class3.csv', index=False)\n",
    "starting_class4.to_csv('../data/starting_class4.csv', index=False)\n",
    "starting_class5.to_csv('../data/starting_class5.csv', index=False)\n",
    "starting_class6.to_csv('../data/starting_class6.csv', index=False)\n",
    "\n",
    "final1_class1.to_csv('../data/final1_class1.csv', index=False)\n",
    "final1_class2.to_csv('../data/final1_class2.csv', index=False)\n",
    "final2_class1.to_csv('../data/final2_class1.csv', index=False)\n",
    "final2_class2.to_csv('../data/final2_class2.csv', index=False)\n",
    "final3_class1.to_csv('../data/final3_class1.csv', index=False)\n",
    "final3_class2.to_csv('../data/final3_class2.csv', index=False)\n",
    "final4_class1.to_csv('../data/final4_class1.csv', index=False)\n",
    "final4_class2.to_csv('../data/final4_class2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f8667-2148-45af-8449-8f3a30d502aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_class5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6cae57-005b-4635-a804-b5ee1be4732c",
   "metadata": {},
   "source": [
    "#### 3) Installing FlashWeave, and making it work. So far, it seems to outperform other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c90de-5228-4d02-86a8-e1893342d8f1",
   "metadata": {},
   "source": [
    "Installed FlashWeave.\n",
    "FlashWeave is programmed within Julia - please make sure to switch to the Julia kernel to run the below code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d064e6-6323-4c1f-89eb-780b9ae1ef45",
   "metadata": {},
   "source": [
    "### 2. Infer networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab99ce7-668d-41f8-bbe5-f39d58a8754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Loading data ###\n",
      "\n",
      "### Normalizing ###\n",
      "\n",
      "Removing variables with 0 variance (or equivalently 1 level) and samples with 0 reads\n",
      "\t-> discarded 0 samples and 567 variables\n",
      "\n",
      "Normalization\n",
      "\n",
      "### Learning interactions ###\n",
      "\n",
      "Inferring network with FlashWeave - sensitive (conditional)\n",
      "\n",
      "\tRun information:\n",
      "\tsensitive - true\n",
      "\theterogeneous - false\n",
      "\tmax_k - 3\n",
      "\talpha - 0.01\n",
      "\tsparse - false\n",
      "\tworkers - 1\n",
      "\tOTUs - 901\n",
      "\tMVs - 0\n",
      "\n",
      "Automatically setting 'n_obs_min' to 20 for enhanced reliability\n",
      "Computing univariate associations\n",
      "\n",
      "Univariate degree stats:\n",
      "Summary Stats:\n",
      "Length:         901\n",
      "Missing Count:  0\n",
      "Mean:           27.738069\n",
      "Minimum:        1.000000\n",
      "1st Quartile:   13.000000\n",
      "Median:         20.000000\n",
      "3rd Quartile:   36.000000\n",
      "Maximum:        129.000000\n",
      "\n",
      "\n",
      "\n",
      "Starting conditioning search\n",
      "\n",
      "Preparing workers..\n",
      "\n",
      "Done. Starting inference..\n",
      "Starting convergence checks at 2437 edges.\n",
      "Latest convergence step change: 0.40525\n",
      "Latest convergence step change: 0.47242\n",
      "Latest convergence step change: 0.08321\n",
      "\n",
      "Postprocessing\n",
      "Complete\n",
      "\n",
      "Finished inference. Total time taken: 7.426s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Mode:\n",
       "FlashWeave - sensitive (conditional)\n",
       "\n",
       "Network:\n",
       "2692 interactions between 901 variables (901 OTUs and 0 MVs)\n",
       "\n",
       "Unfinished variables:\n",
       "none\n",
       "\n",
       "Rejections:\n",
       "not tracked\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using FlashWeave\n",
    "data_path = \"../data/final1_class1.csv\"\n",
    "netw_results = learn_network(data_path, sensitive=true, heterogeneous=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e519862-cdcd-4551-b1a0-870d53a548e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_network(\"../data/final1_class1_network_output.edgelist\", netw_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968ec920-5979-4800-b495-4f54042eb91d",
   "metadata": {},
   "source": [
    "### Applying functionInk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363dfa3d-6116-4b58-aca0-981720be94da",
   "metadata": {},
   "source": [
    "First, modifying the format of the output of FlashWeave to match the input format for functionInk.\n",
    "Please switch to the Python kernel to run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af29737d-c373-45a5-b39a-1647ad15d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Importing Pandas again, as switched back to Python kernel\n",
    "\n",
    "# Removing the headers\n",
    "with open('../data/final1_class1_network_output.edgelist', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "with open('../data/final1_class1_network_output.edgelist', 'w') as f:\n",
    "    f.writelines(lines[2:])\n",
    "\n",
    "# Adding new headers and a column for the type if interaction (here, all assumed to be the same)\n",
    "final1_class1_network_data = pd.read_csv(\"../data/final1_class1_network_output.edgelist\", sep=\"\\t\", header=None, names=[\"ASV_A\", \"ASV_B\", \"Interaction\"])\n",
    "final1_class1_network_data['Type'] = 1\n",
    "\n",
    "# Outputting as a .tsv\n",
    "final1_class1_network_data.to_csv('../data/final1_class1_network_data.tsv', sep='\\t', index=False, header=['#ASV_A', 'ASV_B', 'Interaction', 'Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54d2c0-3dc6-4d0f-89d7-9ceac8b53a0e",
   "metadata": {},
   "source": [
    "Next, running the detailed pipeline for functionInk, as described in its vignette. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6390fa-3561-42d1-ace1-7bc8004ee288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/functionInk # Moving to the functionInk repository to run its commands'\n",
      "/home/matthew/Documents/ResearchProject/ResearchProjectRepository/code\n"
     ]
    }
   ],
   "source": [
    "%cd functionInk # Moving to the functionInk repository to run its commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72362b6b-70dd-43ff-bd73-2c4e36e33e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step to the pipeline - computing similarities between nodes\n",
    "!./NodeSimilarity.pl -w 1 -d 0 -t 0 -f ../../data/final1_class1_network_data.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4271a8-0baa-4262-a74d-618e91215d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second step - clustering nodes using the similarity metrics calculated\n",
    "!./NodeLinkage.pl -fn ../../data/final1_class1_network_data.tsv -fs Nodes-Similarities_final1_class1_network_data.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6909a0-7c40-4fa3-8931-46fca2162c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b975579-2bf7-4dae-a6b1-a7e469cd50c4",
   "metadata": {},
   "source": [
    "The code in the below cell is copied and modified from nodeLinkage_analysis.R found within the functionInk repository. Please switch to the R kernel to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b93ea-16f9-4b46-b5a1-afc7f3965c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third step - identifying the optimal partition\n",
    "\n",
    "library(ggplot2)\n",
    "library(here)\n",
    "\n",
    "# --- Path to dir of history compact file\n",
    "dir=\".\" #  path to dir of history compact file relative to the root of the repo (e.g. fix to \".\" if it is located in the root of the directory)\n",
    "\n",
    "# --- Name of the history file\n",
    "file.hist=\"HistCompact-NL_Average_NoStop_final1_class1_network_data.tsv\" #\"history file name\"\n",
    "\n",
    "src.dir=here(\"scripts\",\"analysis_R\")\n",
    "root.dir=here()\n",
    "setwd(src.dir)\n",
    "source(\"extractPartDensity.R\")\n",
    "if(dir == \".\"){\n",
    "  setwd(root.dir)\n",
    "}else{\n",
    "  setwd(here(dir))\n",
    "}\n",
    "\n",
    "hist.comp=read.table(file=file.hist,sep=\"\\t\",header = TRUE) # for current NodeLink.pl version (Dec 2018)\n",
    "\n",
    "part_density=extractPartDensity(hist.comp)\n",
    "\n",
    "part_density$total_dens # maximum of the total partition density\n",
    "part_density$int_dens # maximum of the internal partition density\n",
    "part_density$ext_dens # maximum of the external partition density\n",
    "part_density$total_dens_step # step of the clustering in which the maximum of the total partition density was found\n",
    "part_density$int_dens_step # step of the clustering in which the maximum of the internal partition density was found\n",
    "part_density$ext_dens_step # step of the clustering in which the maximum of the external partition density was found\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96895a59-ebf6-4eac-94cc-754e4a0f56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving some of the output files to the data directory\n",
    "import os\n",
    "source_file = 'Nodes-Similarities_final1_class1_network_data.tsv'\n",
    "destination_directory = '../../data'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'HistCompact-NL_Average_NoStop_final1_class1_network_data.tsv'\n",
    "destination_directory = '../../data'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "source_file = 'Nodes-Similarities_final1_class1_network_data.tsv'\n",
    "destination_directory = '../../data'\n",
    "file_name = os.path.basename(source_file)\n",
    "destination_path = os.path.join(destination_directory, file_name)\n",
    "os.rename(source_file, destination_path)\n",
    "\n",
    "# Moving the final output files to the results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1cedfe-1b04-4f2b-aba0-a4ce48e312ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
