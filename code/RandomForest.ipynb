{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e915e3c1-c41f-438e-ad3e-f27cc50c6d93",
   "metadata": {},
   "source": [
    "# Multi-Output Random Forest Regression\n",
    "AIM: Identify the role of the abundances of clusters in the starting samples in determining the abundances of clusters in the final samples.\n",
    "\n",
    "STEPS:\n",
    "1. Rarefy ASV table to minimum number of reads.\n",
    "2. Create input dataframes for multioutput random forest regression. Combine asv table, metadata, and cluster data so that there is are two data frames: one with the abundance of each cluster in the starting community, and another with the average abundance of each cluster across the replicates of each final community. The rows should be aligned (such that row 1 in the first data frame is for the parent sample of row 1 in the second data frame).\n",
    "3. Perform multi-output random forest regression using scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba197bd-470f-4c0b-8306-e9ffe4e4ae63",
   "metadata": {},
   "source": [
    "### 1 - Rarefy to minimum number of reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2667e18-782a-4716-96c7-51430724d960",
   "metadata": {},
   "source": [
    "Importing libraries and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809fedad-d9c0-4afe-9730-edd5f5e2a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from biom import Table\n",
    "from biom.util import biom_open\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing data\n",
    "asv_table = pd.read_csv('../data/seqtable_readyforanalysis.csv', index_col=0, delimiter='\\t')\n",
    "cluster_data = pd.read_csv(\"../data/max_tot_ext_network_table.tsv\", delimiter='\\t')\n",
    "meta_data = pd.read_csv(\"../data/metadata_Time0D-7D-4M_May2022_wJSDpart_ext.csv\", delimiter='\\t')\n",
    "taxonomy_data = pd.read_csv(\"../data/taxa_wsp_readyforanalysis.csv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65392807-93cc-4a29-8e08-4c06ab46e55b",
   "metadata": {},
   "source": [
    "Visualising the reads to decide the rarefaction depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a665088-27ec-498f-8b34-ef21b3308272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating number of reads\n",
    "sample_read_counts = asv_table.sum(axis=1) # Number of reads of each samples\n",
    "min_reads = asv_table.sum(axis=1).min() # Minimum number of reads\n",
    "print(f\"Minimum number of reads across samples: {min_reads}\")\n",
    "num_samples_below_10000 = (sample_read_counts < 10000).sum() # Number of samples with reads less than 10,000\n",
    "print(f\"Number of samples with less than 10,000 reads: {num_samples_below_10000}\")\n",
    "\n",
    "# Plotting number of reads\n",
    "plt.hist(sample_read_counts, bins=50)\n",
    "plt.xlabel('Read Counts')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Distribution of Read Counts Across Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628677c1-f478-4e90-91d3-aa0aee6737e9",
   "metadata": {},
   "source": [
    "Filtering out samples with less than 10,000 reads from the asv table and meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72aec6-9b3a-4f5a-a488-99e60ab89c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out samples with less than 10,000 reads and removing experiment 4M samples (different experiment)\n",
    "\n",
    "asv_table['reads'] = asv_table.sum(axis=1) # Column with number of reads for each sample\n",
    "asv_table.reset_index(inplace=True) # Making the sample ID into a column for the ASV table\n",
    "asv_table.rename(columns={'index': 'sampleid'}, inplace=True) # renaming this new column to 'sampleid'\n",
    "\n",
    "indices = asv_table.index[asv_table['reads'] < 10000].tolist() # Indices of samples with less than 10,000 reads\n",
    "asv_table = asv_table.drop(indices)\n",
    "meta_data = meta_data.drop(indices)\n",
    "\n",
    "indices = meta_data.index[meta_data['Experiment'] == '4M'].tolist()\n",
    "asv_table = asv_table.drop(indices)\n",
    "meta_data = meta_data.drop(indices)\n",
    "\n",
    "asv_table.set_index('sampleid', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eaec32-b388-483c-b32f-3dc9520d6d8f",
   "metadata": {},
   "source": [
    "Rarefying the asv table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9bc54c-2c16-4dd7-90a1-9126830edbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting rarefaction depth to 10,000\n",
    "rarefaction_depth = 10000\n",
    "\n",
    "# Function to rarefy a sample\n",
    "def rarefy_vector(v, depth):\n",
    "    non_zero_indices = np.nonzero(v)[0] # Identify indices of non-zero ASVs, as these are the ones with reads\n",
    "    probabilities = v[non_zero_indices] / v.sum() # Calculating probabilities of selecting each non-zero ASV\n",
    "    subsampled = np.random.choice(non_zero_indices, size=depth, replace=True, p=probabilities)  # Randomly subsample the non-zero indices to depth\n",
    "    rarefied = np.zeros_like(v) # New vector with the same size as the original\n",
    "    np.add.at(rarefied, subsampled, 1) # Increment the counts in the rarefied vector based on the subsampling\n",
    "    return rarefied\n",
    "\n",
    "# Function to rarefy the ASV table (where rows are samples)\n",
    "def rarefy_table(df, depth):\n",
    "    rarefied_data = df.apply(lambda x: rarefy_vector(x.values, depth), axis=1) # Applies function that rarefies a sample to each row of the asv table\n",
    "    rarefied_df = pd.DataFrame(rarefied_data.tolist(), index=df.index, columns=df.columns) # \n",
    "    return rarefied_df\n",
    "\n",
    "# Rarefying the asv table\n",
    "asv_table = rarefy_table(asv_table, rarefaction_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957864b8-02ff-463c-a53a-a7f6cb9d2d30",
   "metadata": {},
   "source": [
    "### 2 - Create input dataframes for multioutput random forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ce374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposing so that there is a column for ASVs\n",
    "transposed_asv_table = asv_table.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ebb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging with the cluster data by ASV, such that there is a single cluster column\n",
    "merged_df = transposed_asv_table.merge(cluster_data, left_index=True, right_on='ASV') # Merging by ASV\n",
    "merged_df.drop(['ASV', 'Set.x'], axis=1, inplace=True) # Getting rid of unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a19393-9c71-45f3-aa1a-d6a5b9894e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by cluster and transposing so that each cluster is a column with an abundance in each sample\n",
    "merged_df = merged_df.groupby('functionInk').sum() # Summing all of the rows that have the same cluster value\n",
    "merged_df  = merged_df.transpose() # Transposing so that clusters are columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3fd8ca-82be-4c78-b7c0-bded73c9b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26794b61-30db-47b5-affe-d5946e4aa020",
   "metadata": {},
   "outputs": [],
   "source": [
    "d4=meta_data\n",
    "# Applying the lambda function to create the 'descendant' column\n",
    "d4['descendant'] = d4.apply(lambda row: (row['parent'] + '_descendant') if row['sampleid'] != row['parent'] else row['sampleid'], axis=1)\n",
    "\n",
    "# Selecting the required columns\n",
    "d4 = d4[['sampleid', 'descendant']]\n",
    "\n",
    "\n",
    "# Keeping only 'sampleid' and 'descendant' columns in d4\n",
    "print(d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f12031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging it into the main data frame\n",
    "result_df = merged_df.merge(d4, left_index=True, right_on='sampleid')\n",
    "result_df.drop(['sampleid'], axis=1, inplace=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38229538-14c1-4ed3-b50c-7003ec0a6d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the mean of the cluster abundances across the replicates\n",
    "result_df = result_df.groupby('descendant').mean()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf33a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposing so that clusters are rows\n",
    "transposed_result_df = result_df.transpose()\n",
    "transposed_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d904eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating rows of parents and final samples\n",
    "descendant_rows = result_df[result_df.index.str.contains('descendant')]\n",
    "non_descendant_rows = result_df[~result_df.index.str.contains('descendant')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add k to cluster columns in final samples\n",
    "descendant_rows.columns = [str(col)+'k' for col in descendant_rows.columns]\n",
    "descendant_rows.index = [idx.replace('_descendant', '') for idx in descendant_rows.index]\n",
    "descendant_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add p to cluster columns in parent samples\n",
    "non_descendant_rows.columns = [str(col)+'p'  for col in non_descendant_rows.columns]\n",
    "non_descendant_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ddf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging back into complete data frame\n",
    "complete_df = non_descendant_rows.merge(descendant_rows, left_index=True, right_index=True)\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e8c82-8cc9-4be0-9591-8fe9dc55206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "k_columns = sorted([col for col in complete_df.columns if col.endswith('k')], key=lambda x: int(x[:-1]))\n",
    "p_columns = sorted([col for col in complete_df.columns if col.endswith('p')], key=lambda x: int(x[:-1]))\n",
    "parent_df = complete_df[p_columns]\n",
    "parent_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965a2ab-4728-440e-8555-ecef1e3a018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "descendant_df = complete_df[k_columns]\n",
    "descendant_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6f33f8-7564-40d4-b7c4-80f3caab2593",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54eb6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Loading data\n",
    "predictors_df = parent_df\n",
    "response_df = descendant_df\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "predictors_df_train, predictors_df_test, response_df_train, response_df_test = train_test_split(predictors_df, response_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(predictors_df_train, response_df_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "response_df_pred = model.predict(predictors_df_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(response_df_test, response_df_pred, multioutput='raw_values')\n",
    "print(\"Mean Squared Error for each target:\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
